{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zf/anaconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/zf/pycharm/DecentralizedLLM\")\n",
    "\n",
    "import torch\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from simple_pir.pir import PIRClient\n",
    "\n",
    "from desi_llm.glm6b.obfuscated_layer import WrappedGLMBlock\n",
    "\n",
    "from desi_llm.nodes.computation_node import ComputationNode\n",
    "from desi_llm.nodes.model_provider import ModelProvider\n",
    "from desi_llm.nodes.obfuscator import ObfuscatorNode\n",
    "from llm_bases.chatglm6b import ChatGML6B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from desi_llm.secure_inference import NetworkSimulator, DesiLLMConfig, DesiLLM\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "del sys.modules['desi_llm.secure_inference'] \n",
    "from desi_llm.secure_inference import DesiLLM\n",
    "desi_llm.retrieve_word_embedding = partial(DesiLLM.retrieve_word_embedding, desi_llm)\n",
    "desi_llm.main_body_inference = partial(DesiLLM.main_body_inference, desi_llm)\n",
    "desi_llm.generate = partial(DesiLLM.generate, desi_llm)\n",
    "desi_llm.get_next_token_id = partial(DesiLLM.get_next_token_id, desi_llm)\n",
    "del sys.modules['desi_llm.glm6b.utils']\n",
    "del sys.modules['desi_llm.nodes.computation_node']\n",
    "from desi_llm.nodes.computation_node import ComputationNode\n",
    "for c in desi_llm.computation_nodes:\n",
    "    c.reset_cache = partial(ComputationNode.reset_cache, c)\n",
    "    c.forward_pass = partial(ComputationNode.forward_pass, c)\n",
    "    c.reset_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the original ChatGLM6B model to memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:24<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize model provider...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [01:01<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model provider setup PIR sever...\n",
      "User creating PIR client...\n",
      "Start to load saved obfuscated models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [00:31,  1.12s/it]\n",
      "28it [00:07,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to load models to the designated device...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:05<00:00,  4.97it/s]\n",
      "100%|██████████| 28/28 [00:15<00:00,  1.80it/s]\n",
      "100%|██████████| 56/56 [00:17<00:00,  3.19it/s]\n"
     ]
    }
   ],
   "source": [
    "network_simulator = NetworkSimulator(0.01, 100 * 1024 * 1024)\n",
    "desi_llm = DesiLLM(network_simulator, DesiLLMConfig(\"cuda:1\", 10, 0.1, 10, 0, 10, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids, position_ids, attention_mask = desi_llm.glm6b.get_tokenization(\"Hello, my favorite player is Steven Curry.?\")\n",
    "position_ids = position_ids.to(desi_llm.config.device)\n",
    "attention_mask = attention_mask.to(desi_llm.config.device)\n",
    "token_ids = token_ids[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "e0, e1 = desi_llm.retrieve_word_embedding(token_ids)\n",
    "result_rlcs, key = desi_llm.main_body_inference(e0, e1, position_ids, attention_mask)\n",
    "desi_llm.get_next_token_id(result_rlcs, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zf/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:717: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  tensor = as_tensor(value)\n",
      " 96%|█████████▋| 27/28 [00:03<00:00,  7.40it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.99 GiB. GPU 0 has a total capacty of 23.65 GiB of which 648.06 MiB is free. Process 1134823 has 992.00 MiB memory in use. Process 1135010 has 992.00 MiB memory in use. Process 1136098 has 3.63 GiB memory in use. Including non-PyTorch memory, this process has 17.43 GiB memory in use. Of the allocated memory 16.96 GiB is allocated by PyTorch, and 18.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdesi_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m请列举出几个对中国近代历史影响最大的人\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pycharm/DecentralizedLLM/desi_llm/secure_inference.py:292\u001b[0m, in \u001b[0;36mDesiLLM.generate\u001b[0;34m(self, query, max_len)\u001b[0m\n\u001b[1;32m    290\u001b[0m e0, e1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve_word_embedding(input_ids)\n\u001b[1;32m    291\u001b[0m prediction_rlcs, prediction_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmain_body_inference(e0, e1, position_ids, attention_mask)\n\u001b[0;32m--> 292\u001b[0m next_token_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_next_token_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction_rlcs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28mprint\u001b[39m(next_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglm6b\u001b[38;5;241m.\u001b[39mdecode(next_token_id))\n\u001b[1;32m    294\u001b[0m all_ids\u001b[38;5;241m.\u001b[39mappend(next_token_id\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/pycharm/DecentralizedLLM/desi_llm/secure_inference.py:274\u001b[0m, in \u001b[0;36mDesiLLM.get_next_token_id\u001b[0;34m(self, rlcs, key)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03mrlcs: [seq_len, n_random_vectors, model_dim]\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    273\u001b[0m random_embeddings \u001b[38;5;241m=\u001b[39m rlcs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 274\u001b[0m random_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglm6b\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcondgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m(random_embeddings)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;66;03m# [n_random_vectors, n_tokens]\u001b[39;00m\n\u001b[1;32m    276\u001b[0m recovered_logits \u001b[38;5;241m=\u001b[39m key[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m@\u001b[39m random_logits\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:992\u001b[0m, in \u001b[0;36mModule.float\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m: T) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    984\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Casts all floating point parameters and buffers to ``float`` datatype.\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \n\u001b[1;32m    986\u001b[0m \u001b[38;5;124;03m    .. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 992\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:992\u001b[0m, in \u001b[0;36mModule.float.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m: T) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    984\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Casts all floating point parameters and buffers to ``float`` datatype.\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \n\u001b[1;32m    986\u001b[0m \u001b[38;5;124;03m    .. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 992\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;28;01melse\u001b[39;00m t)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.99 GiB. GPU 0 has a total capacty of 23.65 GiB of which 648.06 MiB is free. Process 1134823 has 992.00 MiB memory in use. Process 1135010 has 992.00 MiB memory in use. Process 1136098 has 3.63 GiB memory in use. Including non-PyTorch memory, this process has 17.43 GiB memory in use. Of the allocated memory 16.96 GiB is allocated by PyTorch, and 18.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "desi_llm.generate(\"请列举出几个对中国近代历史影响最大的人\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = torch.sum(result_rlcs * key[:, 0, :, None], dim=1, keepdim=True)\n",
    "final_embedding = embedding[-1, 0, :].cpu().float()\n",
    "print(final_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(e0 + e1) @ desi_llm.model_provider.word_embedding_key_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids, position_ids, attention_mask = desi_llm.glm6b.get_tokenization(\"Hello, who are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask, position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
