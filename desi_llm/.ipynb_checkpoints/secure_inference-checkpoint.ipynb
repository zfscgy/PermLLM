{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zf/anaconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/zf/pycharm/DecentralizedLLM\")\n",
    "\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from simple_pir.pir import PIRClient\n",
    "\n",
    "from desi_llm.glm6b.obfuscated_layer import WrappedGLMBlock\n",
    "\n",
    "from desi_llm.nodes.computation_node import ComputationNode\n",
    "from desi_llm.nodes.model_provider import ModelProvider\n",
    "from desi_llm.nodes.obfuscator import ObfuscatorNode\n",
    "from llm_bases.chatglm6b import ChatGML6B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:19<00:00,  2.38s/it]\n",
      "100%|██████████| 28/28 [01:00<00:00,  2.16s/it]\n"
     ]
    }
   ],
   "source": [
    "# ============= Setup stage ===========================================\n",
    "# Load model to the CPU memory\n",
    "glm6b = ChatGML6B()\n",
    "\n",
    "# Model provider work:\n",
    "model_provider = ModelProvider(glm6b)\n",
    "\n",
    "obfuscators = []\n",
    "computation_nodes = []\n",
    "for layer in tqdm.tqdm(glm6b.condgen.transformer.layers):\n",
    "    obfuscators.append(ObfuscatorNode())\n",
    "    computation_nodes.append(ComputationNode(WrappedGLMBlock(layer.layer_id)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model provider setup PIR sever...\n",
      "User creating PIR client...\n"
     ]
    }
   ],
   "source": [
    "print(\"Model provider setup PIR sever...\")\n",
    "# Generte PIR hints\n",
    "obfuscator_share = model_provider.generate_shared_embedding()\n",
    "pir_lwe_mat, pir_hint = model_provider.setup_pir_server()\n",
    "print(\"User creating PIR client...\")\n",
    "pir_client = PIRClient(pir_lwe_mat, pir_hint, model_provider.pir_server.get_scale_factor(), model_provider.pir_server.plain_modulus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from desi_llm.glm6b.obfuscated_layer import keys_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [00:27,  1.00it/s]\n",
      "28it [00:07,  3.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load all the obfuscated models\n",
    "model_save_dir = \"./saved_model/\"\n",
    "\n",
    "for i, computation_node in tqdm.tqdm(enumerate(computation_nodes)):\n",
    "    computation_node.layer.load_state_dict(torch.load(model_save_dir + f\"wrappedGLM_{i}.pth\", map_location=\"cpu\"))\n",
    "\n",
    "for i, obfuscator in tqdm.tqdm(enumerate(obfuscators)):\n",
    "    obfuscator.key = torch.load(model_save_dir + f\"obfuscatorKey_{i}.pth\", map_location=\"cpu\")\n",
    "    model_provider.obfuscation_nodes[i].key = torch.load(model_save_dir + f\"providerKey_{i}.pth\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Re-generate all the obfuscations (!!! Taking a long time, can be hours !!!)\n",
    "    print(\"Model provider generating obfuscated layers...\")\n",
    "    obfuscated_layers = model_provider.generate_obfuscations(\"cuda\")\n",
    "\n",
    "    # Set up obfuscator nodes and computation nodes\n",
    "    obfuscators = []\n",
    "    computation_nodes = []\n",
    "    for obfuscated_layer in tqdm.tqdm(obfuscated_layers):\n",
    "        obfuscator = ObfuscatorNode()\n",
    "        obfuscators.append(obfuscator)\n",
    "        # Twice-obfuscation\n",
    "        computation_nodes.append(ComputationNode(obfuscator.obfuscate(obfuscated_layer)))\n",
    "\n",
    "    # Save all the obfuscated models\n",
    "    model_save_dir = \"./saved_model/\"\n",
    "\n",
    "    for i, computation_node in tqdm.tqdm(enumerate(computation_nodes)):\n",
    "        torch.save(computation_node.layer.state_dict(), model_save_dir + f\"wrappedGLM_{i}.pth\")\n",
    "\n",
    "    for i, obfuscator in tqdm.tqdm(enumerate(obfuscators)):\n",
    "        torch.save(obfuscator.key, model_save_dir + f\"obfuscatorKey_{i}.pth\")\n",
    "        torch.save(model_provider.obfuscation_nodes[i].key, model_save_dir + f\"providerKey_{i}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed here, 2024.1.17 17:24\n"
     ]
    }
   ],
   "source": [
    "print(\"Executed here, 2024.1.17 17:24\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:1\"\n",
    "for i in range(len(model_provider.input_transformations)):\n",
    "    model_provider.input_transformations[i] = model_provider.input_transformations[i].half().to(device)\n",
    "for node in computation_nodes:\n",
    "    node.layer = node.layer.half().to(device)\n",
    "for o in model_provider.obfuscation_nodes + obfuscators:\n",
    "    o.key = keys_to_tensor(o.key, float_type=torch.half, int_type=torch.int)\n",
    "    o.key.qkv = [[e1.to(device), e2.to(device)] for e1, e2 in o.key.qkv]\n",
    "    o.key.mlp_output = o.key.mlp_output.to(device)\n",
    "    o.key.attn_out=  o.key.attn_out.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_provider.word_embedding_key = torch.tensor(model_provider.word_embedding_key).half().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from desi_llm.secure_inference import NetworkSimulator\n",
    "ns = NetworkSimulator(0.01, 100 * 1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ids [19316, 6, 172, 118, 120, 31, 130001, 130004]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zf/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:717: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  tensor = as_tensor(value)\n"
     ]
    }
   ],
   "source": [
    "token_ids, position_ids, attention_masks = glm6b.get_tokenization(\"Hello, who are you?\")\n",
    "token_ids = token_ids[0].tolist()\n",
    "print(\"Token ids\", token_ids)\n",
    "position_ids = position_ids.to(device)\n",
    "attention_masks = attention_masks.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The user query the word embeddings\n",
    "\n",
    "token_ids += [i + glm6b.max_token_id for i in token_ids]\n",
    "pir_queries = []\n",
    "for token_id in token_ids:\n",
    "    pir_queries.append(pir_client.query(token_id))\n",
    "\n",
    "ns.transfer(np.array(pir_queries), \"pir: client queries\")\n",
    "\n",
    "pir_answers = []\n",
    "for q in pir_queries:\n",
    "    pir_answers.append(model_provider.pir_server.answer(q))\n",
    "    \n",
    "\n",
    "ns.transfer(np.array(pir_answers), \"pir: server answers\")\n",
    "\n",
    "recovered_perm_ids = []\n",
    "for token_id, a in zip(token_ids, pir_answers):\n",
    "    recovered_perm_ids.append(pir_client.recover(token_id, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 64656  50031  11507  15176  51657  70418 126738 107543]\n",
      "[ 64656  50031  11507  15176  51657  70418 126738 107543]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "recovered_perm_ids = np.array(recovered_perm_ids)\n",
    "recovered_perm_ids = recovered_perm_ids[:len(recovered_perm_ids) // 2] * 1000 + recovered_perm_ids[len(recovered_perm_ids) // 2:]\n",
    "print(recovered_perm_ids)\n",
    "print(model_provider.permutation[token_ids[:8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current communication and time: 0.12 MB, 0.02 s\n"
     ]
    }
   ],
   "source": [
    "# The PIR phase\n",
    "\n",
    "print(f\"Current communication and time: {ns.total_comm / (1024 ** 2):.2f} MB, {ns.total_time:.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from desi_llm.common.utils import generate_random_linear_combination, generate_random_transformations\n",
    "n_random_vectors = 1\n",
    "seq_len = len(recovered_perm_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current communication and time: 0.12 MB, 0.03 s\n"
     ]
    }
   ],
   "source": [
    "# The user first reconstruct the shares of word embeddings\n",
    "from desi_llm.glm6b.configs import GLM6BConfig\n",
    "from desi_llm.common.utils import random_vec_with_seed, generate_random_linear_combination\n",
    "\n",
    "import torch\n",
    "\n",
    "embedding_share_0 = []\n",
    "\n",
    "for perm_id in recovered_perm_ids:\n",
    "    embedding_share_0.append(random_vec_with_seed(perm_id, GLM6BConfig.model_dim, [-1, 1]))\n",
    "\n",
    "embedding_share_0 = np.array(embedding_share_0)\n",
    "# The user re-mask the word embedding\n",
    "embedding_share_0 += random_vec_with_seed(19260817, embedding_share_0.shape, [-1, 1])\n",
    "# The user generate the linear combinations\n",
    "\n",
    "# random_transformation = generate_random_transformations(seq_len, n_random_vectors)\n",
    "random_transformation = (torch.ones(seq_len, 1, 1), torch.ones(seq_len, 1, 1))\n",
    "\n",
    "rlcs_0 = generate_random_linear_combination(torch.tensor(embedding_share_0)[:, None, :].half().to(device), \n",
    "                                            n_random_vectors, random_transformation[0].half().to(device))\n",
    "# The user sends the recovered permutation to the obfuscator, \n",
    "# along with the transformation matrices so that they both maintain a share of the random vector\n",
    "\n",
    "\n",
    "ns.transfer(np.array(recovered_perm_ids), \"word embedding: user recovered permutation\")\n",
    "print(f\"Current communication and time: {ns.total_comm / (1024 ** 2):.2f} MB, {ns.total_time:.2f} s\")\n",
    "\n",
    "# The transformation can also be synced via a random seed\n",
    "embedding_share_1 = obfuscator_share[recovered_perm_ids]\n",
    "embedding_share_1 -= random_vec_with_seed(19260817, embedding_share_1.shape, [-1, 1])\n",
    "\n",
    "\n",
    "rlcs_1 = torch.tensor(embedding_share_1)[:, None, :].half().to(device)\n",
    "rlcs_0 = rlcs_0 @ model_provider.word_embedding_key\n",
    "rlcs_1 = rlcs_1 @ model_provider.word_embedding_key\n",
    "\n",
    "random_transformation = (random_transformation[0].half().to(device), random_transformation[1].half().to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual=============\n",
      " tensor([[[-1.5605e+00, -3.8086e-02, -1.5391e+00,  ..., -1.7393e+00,\n",
      "           6.7529e-01,  1.2617e+00]],\n",
      "\n",
      "        [[ 1.8203e+00, -6.6309e-01,  3.1104e-01,  ...,  1.7871e-01,\n",
      "           2.0820e+00, -1.5225e+00]],\n",
      "\n",
      "        [[ 6.5527e-01,  6.3086e-01, -1.5645e+00,  ..., -1.2881e+00,\n",
      "          -1.5271e-01,  1.0439e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 6.5869e-01,  2.7930e-01,  7.4512e-01,  ..., -3.2837e-01,\n",
      "           9.6094e-01,  5.5762e-01]],\n",
      "\n",
      "        [[ 1.8359e-01, -3.9014e-01,  9.5625e+00,  ..., -7.4414e-01,\n",
      "          -2.2754e-01,  1.5552e-01]],\n",
      "\n",
      "        [[ 2.7441e-01, -5.3467e-02, -1.6772e-01,  ...,  1.7529e-01,\n",
      "          -8.9111e-03, -1.7676e-01]]], device='cuda:1', dtype=torch.float16)\n",
      "Output==============\n",
      " tensor([[[-1.2256,  0.3003, -1.9492,  ..., -2.1230,  0.7051,  1.2422]],\n",
      "\n",
      "        [[ 0.7681, -0.2625, -0.0475,  ...,  0.2107,  0.7202, -0.9814]],\n",
      "\n",
      "        [[ 0.5845,  0.4648, -0.7349,  ..., -0.5518, -0.9180,  0.9951]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0180,  0.0241,  0.1892,  ...,  0.2820,  0.1628,  0.1159]],\n",
      "\n",
      "        [[-0.0119, -0.2588,  4.5117,  ..., -0.5410,  0.0531,  0.1086]],\n",
      "\n",
      "        [[ 0.4199, -0.0446, -0.3069,  ...,  0.4136,  0.0220, -0.2515]]],\n",
      "       device='cuda:1', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# rlcs_1 += rlcs_0\n",
    "# rlcs_0 -= rlcs_0\n",
    "for i in tqdm.tqdm(range(len(obfuscators))):\n",
    "    with torch.no_grad():\n",
    "        qkv_rlcs_proj = model_provider.input_transformations[i](rlcs_0, position_ids, only_projection=True)\n",
    "        qkv_rlcs_tran = model_provider.input_transformations[i](rlcs_1, position_ids)\n",
    "        # 3 * [seq_len, k, n_heads, head_dim]\n",
    "        \n",
    "        residual_proj = model_provider.input_transformations[i](rlcs_0, position_ids, only_affine=True, only_projection=True)\n",
    "        residual_tran = model_provider.input_transformations[i](rlcs_1, position_ids, only_affine=True)\n",
    "\n",
    "\n",
    "    qkv_rlcs = list(qkv_rlcs_proj) + list(qkv_rlcs_tran)\n",
    "    # print(qkv_rlcs[0] + qkv_rlcs[3])\n",
    "    qkv_rlcs = list(model_provider.obfuscation_nodes[i].forward_pass(qkv_rlcs[:3], \"qkv\")) + \\\n",
    "               list(model_provider.obfuscation_nodes[i].forward_pass(qkv_rlcs[3:], \"qkv\"))\n",
    "    # (2 * 3(q, k, v)) * [seq_len, k, n_heads, head_dim]\n",
    "\n",
    "    \n",
    "    residual_rlcs = [residual_proj, residual_tran]\n",
    "    residual_rlcs = [model_provider.obfuscation_nodes[i].forward_pass(rlc, \"attn_out\") for rlc in residual_rlcs]\n",
    "    # (2 * seq_len) * [k, model_dim]\n",
    "    \n",
    "\n",
    "    # send rlcs to the obfuscator\n",
    "    ns.transfer(qkv_rlcs + residual_rlcs, f\"forward embedding: send to obfusctor {i}\")\n",
    "    \n",
    "    qkv_rlcs = list(obfuscators[i].forward_pass(qkv_rlcs[:3], \"qkv\")) + list(obfuscators[i].forward_pass(qkv_rlcs[3:], \"qkv\"))\n",
    "    # (2 * 3(q, k, v)) * [seq_len, k, n_heads, head_dim]\n",
    "    residual_rlcs = [obfuscators[i].forward_pass(rlc, \"attn_out\") for rlc in residual_rlcs]\n",
    "    # 2 * [seq_len, k, model_dim]\n",
    "\n",
    "    # send rlcs to the computation node\n",
    "    ns.transfer(qkv_rlcs + residual_rlcs, f\"forward embedding: send to computation node {i}\")\n",
    "\n",
    "\n",
    "    forward_embedding_share_0 = [torch.sum(rlc * random_transformation[1][:, 0, :, None, None], dim=1, keepdim=True) for rlc in qkv_rlcs[:3]]\n",
    "    forward_embedding_share_1 = qkv_rlcs[3:]\n",
    "    # (2 * 3(qkv)) * [seq_len, 1, n_heads, head_dim]\n",
    "    forward_embedding = [a + b for a, b in zip(forward_embedding_share_0, forward_embedding_share_1)]\n",
    "\n",
    "    residual_embedding_share_0 = torch.sum(residual_rlcs[0] * random_transformation[1][:, 0, :, None], dim=1, keepdim=True)\n",
    "    residual_embedding_share_1 = residual_rlcs[1]\n",
    "    # 2 * [seq_len, 1, model_dim]\n",
    "\n",
    "    residual_embedding = residual_embedding_share_0 + residual_embedding_share_1\n",
    "    # [seq_len, 1, model_dim]\n",
    "#     print(\"Residual=============\\n\", residual_embedding)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embedding = computation_nodes[i].forward_pass(forward_embedding, attention_masks, residual_embedding)\n",
    "\n",
    "    # Check the 'raw' embedding\n",
    "#     print(\"Output==============\\n\", embedding)\n",
    "#     input()\n",
    "\n",
    "    # computation node generate linear combinations\n",
    "    embedding_share_0 = 2 * torch.std(embedding) * (torch.rand_like(embedding) - 0.5)\n",
    "    embedding_share_1 = embedding - embedding_share_0\n",
    "\n",
    "    random_transformation = generate_random_transformations(seq_len, n_random_vectors)\n",
    "    random_transformation = random_transformation[0].half().to(device), random_transformation[1].half().to(device)\n",
    "    rlcs_0 = generate_random_linear_combination(embedding_share_0, n_random_vectors, random_transformation[0])\n",
    "    rlcs_1 = embedding_share_1\n",
    "\n",
    "\n",
    "    rlcs_0 = obfuscators[i].forward_pass(rlcs_0, \"mlp_output\", reverse=True)\n",
    "    rlcs_1 = obfuscators[i].forward_pass(rlcs_1, \"mlp_output\", reverse=True)\n",
    "\n",
    "    rlcs_0 = model_provider.obfuscation_nodes[i].forward_pass(rlcs_0, \"mlp_output\", reverse=True)\n",
    "    rlcs_1 = model_provider.obfuscation_nodes[i].forward_pass(rlcs_1, \"mlp_output\", reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = obfuscators[i].forward_pass(embedding, \"mlp_output\", reverse=True)\n",
    "embedding = model_provider.obfuscation_nodes[i].forward_pass(embedding, \"mlp_output\", reverse=True)\n",
    "\n",
    "def recover_final_embedding(embedding: torch.Tensor):\n",
    "    final_embedding = embedding[-1, 0, :].cpu().float()\n",
    "    logits = glm6b.condgen.lm_head.float()(final_embedding)\n",
    "    token_id = torch.argmax(logits).item()\n",
    "    print(token_id)\n",
    "    print(glm6b.tokenizer.decode(token_id))\n",
    "\n",
    "recover_final_embedding(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(qkv_rlcs[0] * random_transformation[1][:, 0, :, None, None], dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del sys.modules['desi_llm.glm6b.obfuscated_layer']\n",
    "del sys.modules['desi_llm.nodes.model_provider']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "del sys.modules['desi_llm.nodes.obfuscator']\n",
    "from desi_llm.nodes.obfuscator import ObfuscatorNode\n",
    "for o in obfuscators:\n",
    "    o.forward_pass = partial(ObfuscatorNode.forward_pass, o)\n",
    "for o in model_provider.obfuscation_nodes:\n",
    "    o.forward_pass = partial(ObfuscatorNode.forward_pass, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sys.modules['desi_llm.nodes.computation_node']\n",
    "from desi_llm.nodes.computation_node import ComputationNode\n",
    "for c in computation_nodes:\n",
    "    c.forward_pass = partial(ComputationNode.forward_pass, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some temporary codes\n",
    "glm6b.tokenizer(\"Thousand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del random_transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seqs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 15,  20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
    "seqs = np.array(seqs)\n",
    "\n",
    "def compute_avg_beats(end_time: float):\n",
    "    num_beats = np.sum((seqs >= end_time - 5) & (seqs <= end_time))\n",
    "    rate = 20 * num_beats\n",
    "    return rate\n",
    "\n",
    "rates = [compute_avg_beats(i) for i in range(5, 30)]\n",
    "plt.plot(rates)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization = glm6b.get_tokenization(\"Hello, who are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = glm6b.get_initial_state(tokenization[0])\n",
    "state_0 = glm6b.condgen.transformer.layers[0].float()(initial_state, tokenization[1], tokenization[2], torch.tensor(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "print(F.layer_norm(initial_state, [4096]))\n",
    "print(state_0)\n",
    "print(torch.tensor(embedding_share_0 + embedding_share_1).float() @ model_provider.word_embedding_key.cpu().float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_embedding = glm6b.condgen.transformer.word_embeddings.weight[:ChatGML6B.max_token_id].numpy().astype(np.float32)[token_ids[:8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(model_provider.shared_rotated_word_embedding[token_ids[:8]] + obfuscator_share[model_provider.permutation[token_ids[:8]]]).half().cuda() @ torch.tensor(model_provider.word_embedding_key).half().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_00 = model_provider.input_transformations[0].cpu().float().projection_part_transform(initial_state, tokenization[1]) + \\\n",
    "           model_provider.input_transformations[0].cpu().float().translation_part_transform(tokenization[1])\n",
    "print(state_00[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_provider.pir_server.data_matrix.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_provider.pir_server.data_matrix.flatten()[19316] * 1000 + model_provider.pir_server.data_matrix.flatten()[19316 + glm6b.max_token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_provider.pir_server.get_scale_factor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from desi_llm.common.utils import random_orthogonal\n",
    "random_orthogonal(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = torch.rand(8, 1, 4096).half().cuda()\n",
    "print(test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_forward_1 = obfuscators[i].forward_pass(test_batch, \"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_recovered = obfuscators[i].forward_pass(test_batch_forward_1, \"v\", reverse=True)\n",
    "print(test_batch_recovered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obfuscators[i].key.mlp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(torch.tensor([[1,2,3]]), dim=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_vec_with_seed(130005, [10], [-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 2, 3, 4, 5])\n",
    "a[[4, 3, 2, 1, 0]] = a\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
