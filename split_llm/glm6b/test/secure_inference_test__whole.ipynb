{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from split_llm.glm6b.wrapped_layer import Attention_GLM_Wrapped, copy_attention, FeedForward_GLM_Wrapped, copy_feedforward\n",
    "from split_llm.glm6b.utils import generate_position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import sys\n",
    "try:\n",
    "    del sys.modules[\"split_llm.glm6b.secure_inference\"]\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:08<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "from llm_bases.chatglm6b import ChatGML6B\n",
    "glm = ChatGML6B()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_bases.chatglm6b_official.modeling_chatglm import GLMBlock\n",
    "\n",
    "\n",
    "raw_glm_layers: List[GLMBlock] = glm.condgen.transformer.layers\n",
    "attentions: List[Attention_GLM_Wrapped] = []\n",
    "attentions_public: List[Attention_GLM_Wrapped] = []\n",
    "ffs: List[FeedForward_GLM_Wrapped] = []\n",
    "for i in range(28):\n",
    "    transformer_layer = raw_glm_layers[i].float()\n",
    "    \n",
    "    # The private attention layer\n",
    "    attn_wrapped = Attention_GLM_Wrapped(4096, 32, i)\n",
    "    copy_attention(transformer_layer, attn_wrapped)\n",
    "    attn_wrapped.requires_grad_(False)\n",
    "    attentions.append(attn_wrapped.cuda())\n",
    "\n",
    "    # The public attention layer\n",
    "    attn_wrapped_public = Attention_GLM_Wrapped(4096, 32, 0)\n",
    "    attn_wrapped_public.qkv_weight = None\n",
    "    attn_wrapped_public.qkv_bias = None\n",
    "    attn_wrapped_public.attn_out_weight = None\n",
    "    attn_wrapped_public.attn_out_bias = None\n",
    "    attn_wrapped_public.positional_embedding = attn_wrapped.positional_embedding\n",
    "    attn_wrapped_public.requires_grad_(False)\n",
    "    attentions_public.append(attn_wrapped_public.cuda())\n",
    "\n",
    "    ff_wrapped = FeedForward_GLM_Wrapped(4096, 32, i)\n",
    "    if i == 27:\n",
    "        copy_feedforward(transformer_layer, None, ff_wrapped)\n",
    "    else:\n",
    "        copy_feedforward(transformer_layer, raw_glm_layers[i + 1].float(), ff_wrapped)\n",
    "    ff_wrapped.requires_grad_(False)\n",
    "    ffs.append(ff_wrapped.cuda())\n",
    "\n",
    "word_embedding = glm.condgen.transformer.word_embeddings.float().cuda()\n",
    "word_embedding.weight.data = raw_glm_layers[0].input_layernorm.float().cuda()(word_embedding.weight.data[:glm.n_tokens])\n",
    "lm_head = glm.condgen.lm_head.float().cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from split_llm.common.communication import Communication, Node, SimulatedCommunication\n",
    "communication = SimulatedCommunication([\"n0\", \"n1\", \"n2\"])\n",
    "communication.new_stage(\"Test\")\n",
    "\n",
    "n0 = Node(communication, \"n0\")\n",
    "n1 = Node(communication, \"n1\")\n",
    "n2 = Node(communication, \"n2\")\n",
    "\n",
    "n0.space.attentions = attentions\n",
    "n1.space.attentions = attentions_public\n",
    "n0.space.ffs = n1.space.ffs = ffs\n",
    "n0.space.word_embedding = word_embedding\n",
    "n0.space.final_dense = lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from split_llm.glm6b.secure_inference import GLM_Protocol\n",
    "\n",
    "whole_protocol = GLM_Protocol(n0, n1, n2, 10, 100, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_protocol.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_protocol.offline_execute(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 130006])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:717: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608935911/work/torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  tensor = as_tensor(value)\n"
     ]
    }
   ],
   "source": [
    "def get_input_tensor(query: str):\n",
    "    input_ids, _, _ = glm.get_tokenization(query)\n",
    "    input_ids = input_ids[0]\n",
    "    input_selector = torch.zeros(len(input_ids), glm.n_tokens)\n",
    "    for i in range(len(input_ids)):\n",
    "        input_selector[i, input_ids[i]] = 1\n",
    "    return input_selector\n",
    "\n",
    "input_tensor = get_input_tensor(\"Hello\")\n",
    "print(input_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1.storage[f\"{whole_protocol.name}:x\"] = input_tensor.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_protocol.online_execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[194]\n"
     ]
    }
   ],
   "source": [
    "print(n1.storage[f\"{whole_protocol.name}:z\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1110]\n"
     ]
    }
   ],
   "source": [
    "index_selector = torch.zeros([1, glm.n_tokens]).cuda()\n",
    "index_selector[0, 1110] = 1\n",
    "n1.storage[f\"{whole_protocol.name}:x\"] = index_selector\n",
    "whole_protocol.offline_execute(1)\n",
    "whole_protocol.online_execute()\n",
    "print(n1.storage[f\"{whole_protocol.name}:z\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is the same as a few months ago ago ago ago ago'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm.decode([194, 107, 100, 254, 114, 104, 437, 589, 1110, 1110, 1110, 1110, 1110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n0.storage[\"transformer_layer_0/attn/dot_product:x-u\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n0.storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(input_tensor, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
