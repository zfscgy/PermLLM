{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:08<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "from llm_bases.chatglm6b import ChatGML6B\n",
    "glm = ChatGML6B()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from split_llm.glm6b.wrapped_layer import Attention_GLM_Wrapped, FeedForward_GLM_Wrapped, copy_attention, copy_feedforward\n",
    "from split_llm.common.torch_utils import relative_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_glm_layers = glm.condgen.transformer.layers\n",
    "attentions: List[Attention_GLM_Wrapped] = []\n",
    "ffs: List[FeedForward_GLM_Wrapped] = []\n",
    "for i in range(28):\n",
    "    transformer_layer = raw_glm_layers[i].float()\n",
    "    attn_wrapped = Attention_GLM_Wrapped(4096, 32, i)\n",
    "    copy_attention(transformer_layer, attn_wrapped)\n",
    "    attentions.append(attn_wrapped.cuda())\n",
    "\n",
    "    ff_wrapped = FeedForward_GLM_Wrapped(4096, 32, i)\n",
    "    if i == 27:\n",
    "        copy_feedforward(transformer_layer, None, ff_wrapped)\n",
    "    else:\n",
    "        copy_feedforward(transformer_layer, raw_glm_layers[i + 1].float(), ff_wrapped)\n",
    "    ffs.append(ff_wrapped.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from split_llm.glm6b.utils import generate_attention_mask, generate_position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the GLM model weights (embeddings at the beginning and the end) into CUDA-float\n",
    "\n",
    "glm.condgen.transformer.word_embeddings.float().cuda()\n",
    "glm.condgen.lm_head.float().cuda()\n",
    "\n",
    "layernorm0 = glm.condgen.transformer.layers[0].input_layernorm.float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_hidden_scales(query: str, generation_length: int):\n",
    "    input_ids, position_ids, attention_masks = glm.get_tokenization(query)\n",
    "    input_ids = input_ids.cuda()\n",
    "    position_ids = position_ids.cuda()\n",
    "    attention_masks = attention_masks.cuda()\n",
    "\n",
    "    original_length = len(input_ids[0])\n",
    "    \n",
    "    all_scales = []\n",
    "    for i in range(generation_length):\n",
    "        # print(f\"Generate {i}-th token\")\n",
    "        # Here the first layernorm is moved\n",
    "        h = layernorm0(glm.get_initial_state(input_ids))\n",
    "        layer_scales = []\n",
    "        for j in range(28):\n",
    "            # print(\"Layer\", j)\n",
    "            # Forward to the i-th attention layer    \n",
    "            scale_h_in = h.abs().max().item()\n",
    "\n",
    "            # Attention module\n",
    "            attn_layer: Attention_GLM_Wrapped = attentions[j]\n",
    "            q, k, v = attn_layer.generate_qkv(h, position_ids)\n",
    "            scale_v = v.abs().max().item()  # Record the scale of V\n",
    "            attn_scores = attn_layer.generate_logit_scores(q, k)\n",
    "            scale_attn_score = attn_scores.abs().max().item()  # Record the scale since here the plaintext is used\n",
    "            softmax_scores = attn_layer.generate_softmax_scores(attn_scores)\n",
    "            weighted_v = attn_layer.generate_weighted_values(softmax_scores, v)\n",
    "            attn_out = weighted_v @ attn_layer.attn_out_weight.T + attn_layer.attn_out_bias\n",
    "            # Feedforward module\n",
    "            ff_layer: FeedForward_GLM_Wrapped = ffs[j]\n",
    "            h0 = ff_layer.layernorm_in(attn_out + h * (2 * 28) ** 0.5)\n",
    "            h1 = ff_layer.mlp_dense_in(h0)\n",
    "            \n",
    "            scale_mlp_hidden = h1.abs().max().item()\n",
    "            h2 = F.gelu(h1)\n",
    "\n",
    "            #  h2 = gelu_openai(h1)\n",
    "            #  Those two gelu implementations do not have significant difference\n",
    "            h3 = ff_layer.mlp_dense_out(h2)\n",
    "            h4 = h3 + ff_layer.residual_coef * h0\n",
    "            h5 = ff_layer.layernorm_out(h4)\n",
    "\n",
    "            h = h5\n",
    "\n",
    "            layer_scales.append([scale_h_in, scale_v, scale_attn_score, scale_mlp_hidden])\n",
    "\n",
    "        logits = glm.condgen.lm_head(h).permute(1, 0, 2).contiguous()[0, -1, :glm.n_tokens]\n",
    "        # Get the logits on the next position\n",
    "        next_id = torch.argmax(logits)\n",
    "        # print(\"Next ID\", next_id)\n",
    "\n",
    "        if next_id == glm.condgen.generation_config.eos_token_id:\n",
    "            break\n",
    "        input_ids = torch.cat([input_ids, torch.tensor([[next_id]]).cuda()], dim=-1)  # Append the last id\n",
    "        new_seq_len = len(input_ids[0])\n",
    "        position_ids = generate_position_ids(original_length, new_seq_len).cuda()\n",
    "        # print(position_ids)\n",
    "        all_scales.append(layer_scales)\n",
    "    \n",
    "    token_ids = input_ids[0].tolist()\n",
    "\n",
    "    print(glm.decode(token_ids))\n",
    "    return np.array(all_scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who are you? I am an AI assistant named ChatGLM-6B, which is developed based on the language model jointly trained by Tsinghua University KEG Lab and Zhipu AI Company in 2023. My job is to provide appropriate answers and support to users' questions and requests.\n"
     ]
    }
   ],
   "source": [
    "scales = analyze_hidden_scales(\"Who are you?\", 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 41.12500763  19.47050858 537.60595703  29.79353333]\n",
      " [ 60.0743866   19.36989403 642.50958252  18.11071968]\n",
      " [ 60.00803757  14.69905281 214.58674622  13.34783936]\n",
      " [ 59.92230606  13.67966652 319.72744751  14.88466072]\n",
      " [ 61.09487915  12.17244339 292.53955078  12.44941902]\n",
      " [ 62.06698227  11.19873619 290.99072266  19.49498749]\n",
      " [ 63.56050873  14.38836288 272.722229    13.93253326]\n",
      " [ 55.89038849  13.50420952 393.37161255  14.81556225]\n",
      " [ 54.33137894  12.17324829 450.45004272  13.13796043]\n",
      " [ 60.39410019  12.61477852 404.82476807  18.34564018]\n",
      " [ 63.29088974  10.14312744 348.49676514  33.34929657]\n",
      " [ 63.05445099  11.6084137  412.60134888  16.34067917]\n",
      " [ 68.44353485  10.68967724 333.69073486  25.55764198]\n",
      " [ 65.80563354  12.22967529 402.78411865  15.27272034]\n",
      " [ 67.4360733   10.89254761 258.52587891  15.60247993]\n",
      " [ 62.42052078  10.16563988 295.77932739  13.91898155]\n",
      " [ 60.87455368  11.46492958 523.45654297  14.8211813 ]\n",
      " [ 54.4107666   10.77226257 172.12240601  13.12202263]\n",
      " [ 52.10651398  11.87335396 183.11177063  17.14702415]\n",
      " [ 48.87733459  13.13734055 251.18275452  18.02664948]\n",
      " [ 47.94239426  13.08519459 260.93896484  17.58930397]\n",
      " [ 53.78763962  14.92441845 258.68249512  24.85417175]\n",
      " [ 58.19782257  12.19316387 195.85496521  19.64110565]\n",
      " [ 64.96772003  10.74958801 310.55343628  26.88719368]\n",
      " [ 59.99866867  15.98443127 329.76922607  26.5527935 ]\n",
      " [ 58.03669739  16.39768982 322.40145874  11.70878792]\n",
      " [ 49.49491501  14.80403614 231.48721313  23.01192093]\n",
      " [ 47.73770523  33.16254044 185.39207458  25.27673149]]\n"
     ]
    }
   ],
   "source": [
    "print(np.max(scales, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_scale = 10000_4214\n",
    "\n",
    "torch.tensor(original_scale + 34.52) - torch.tensor(original_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer  0, QKV max: 0.2466, AttnOut max: 0.6855\n",
      "Layer  1, QKV max: 0.1552, AttnOut max: 0.5874\n",
      "Layer  2, QKV max: 0.1327, AttnOut max: 0.3381\n",
      "Layer  3, QKV max: 0.1476, AttnOut max: 0.4224\n",
      "Layer  4, QKV max: 0.1367, AttnOut max: 0.2632\n",
      "Layer  5, QKV max: 0.1354, AttnOut max: 0.3162\n",
      "Layer  6, QKV max: 0.1494, AttnOut max: 0.6372\n",
      "Layer  7, QKV max: 0.1163, AttnOut max: 0.4460\n",
      "Layer  8, QKV max: 0.1791, AttnOut max: 0.6919\n",
      "Layer  9, QKV max: 0.1908, AttnOut max: 0.8496\n",
      "Layer 10, QKV max: 0.1379, AttnOut max: 0.3650\n",
      "Layer 11, QKV max: 0.1356, AttnOut max: 0.5137\n",
      "Layer 12, QKV max: 0.1398, AttnOut max: 0.3311\n",
      "Layer 13, QKV max: 0.1420, AttnOut max: 0.8291\n",
      "Layer 14, QKV max: 0.1332, AttnOut max: 0.6045\n",
      "Layer 15, QKV max: 0.1268, AttnOut max: 0.8643\n",
      "Layer 16, QKV max: 0.1398, AttnOut max: 0.3745\n",
      "Layer 17, QKV max: 0.1396, AttnOut max: 0.5762\n",
      "Layer 18, QKV max: 0.1377, AttnOut max: 0.3770\n",
      "Layer 19, QKV max: 0.1519, AttnOut max: 0.6987\n",
      "Layer 20, QKV max: 0.1560, AttnOut max: 0.6221\n",
      "Layer 21, QKV max: 0.1562, AttnOut max: 0.6138\n",
      "Layer 22, QKV max: 0.1604, AttnOut max: 0.6016\n",
      "Layer 23, QKV max: 0.1735, AttnOut max: 0.5801\n",
      "Layer 24, QKV max: 0.1919, AttnOut max: 0.5786\n",
      "Layer 25, QKV max: 0.2264, AttnOut max: 0.5513\n",
      "Layer 26, QKV max: 0.1904, AttnOut max: 0.8110\n",
      "Layer 27, QKV max: 0.1741, AttnOut max: 1.1348\n"
     ]
    }
   ],
   "source": [
    "# Check the scale of important weights\n",
    "# This is helpful for deciding the mask_size\n",
    "\n",
    "\n",
    "for i, (attn, ff) in enumerate(zip(attentions, ffs)):\n",
    "    print(f\"Layer {i:2d}, QKV max: {torch.max(attn.qkv_weight).item():4.4f}, AttnOut max: {torch.max(attn.attn_out_weight).item():4.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForward_GLM_Wrapped(\n",
       "  (layernorm_in): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  (mlp_dense_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "  (mlp_dense_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "  (layernorm_out): Identity()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
