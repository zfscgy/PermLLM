{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "from simple_socket.zf_socket import SocketServer\n",
    "from split_llm.common.communication import Node\n",
    "from split_llm.common.real_communication import RealCommunication\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "\n",
    "# Set up communication\n",
    "\n",
    "address_dict = {\n",
    "    \"127.0.0.1:9000\": \"n0\",\n",
    "    \"127.0.0.1:9001\": \"n1\",\n",
    "    \"127.0.0.1:9002\": \"n2\"\n",
    "}\n",
    "sock0 = SocketServer(\"127.0.0.1:9000\", address_dict, 1000)\n",
    "sock1 = SocketServer(\"127.0.0.1:9001\", address_dict, 1000)\n",
    "sock2 = SocketServer(\"127.0.0.1:9002\", address_dict, 1000)\n",
    "\n",
    "time.sleep(1) # Wait the server to start listening\n",
    "\n",
    "sock0.connect_all()\n",
    "sock1.connect_all()\n",
    "sock2.connect_all()\n",
    "\n",
    "comm0 = RealCommunication([\"n0\", \"n1\", \"n2\"], {\"n0\": sock0}, tensor_device=device)\n",
    "comm1 = RealCommunication([\"n0\", \"n1\", \"n2\"], {\"n1\": sock1}, tensor_device=device)\n",
    "comm2 = RealCommunication([\"n0\", \"n1\", \"n2\"], {\"n2\": sock2}, tensor_device=device)\n",
    "\n",
    "n0 = Node(comm0, \"n0\")\n",
    "n1 = Node(comm1, \"n1\")\n",
    "n2 = Node(comm2, \"n2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:09<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from split_llm.glm6b.wrapped_layer import Attention_GLM_Wrapped, copy_attention, FeedForward_GLM_Wrapped, copy_feedforward\n",
    "from split_llm.glm6b.utils import generate_position_ids\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from llm_bases.chatglm6b import ChatGML6B\n",
    "glm = ChatGML6B()\n",
    "\n",
    "from llm_bases.chatglm6b_official.modeling_chatglm import GLMBlock\n",
    "\n",
    "raw_glm_layers: List[GLMBlock] = glm.condgen.transformer.layers\n",
    "attentions: List[Attention_GLM_Wrapped] = []\n",
    "attentions_public: List[Attention_GLM_Wrapped] = []\n",
    "ffs: List[FeedForward_GLM_Wrapped] = []\n",
    "for i in range(28):\n",
    "    transformer_layer = raw_glm_layers[i].float()\n",
    "    \n",
    "    # The private attention layer\n",
    "    attn_wrapped = Attention_GLM_Wrapped(4096, 32, i)\n",
    "    copy_attention(transformer_layer, attn_wrapped)\n",
    "    attn_wrapped.requires_grad_(False)\n",
    "    attentions.append(attn_wrapped.cuda())\n",
    "\n",
    "    # The public attention layer\n",
    "    attn_wrapped_public = Attention_GLM_Wrapped(4096, 32, i)\n",
    "    attn_wrapped_public.qkv_weight = None\n",
    "    attn_wrapped_public.qkv_bias = None\n",
    "    attn_wrapped_public.attn_out_weight = None\n",
    "    attn_wrapped_public.attn_out_bias = None\n",
    "    attn_wrapped_public.positional_embedding = attn_wrapped.positional_embedding\n",
    "    attn_wrapped_public.requires_grad_(False)\n",
    "    attentions_public.append(attn_wrapped_public.cuda())\n",
    "\n",
    "    ff_wrapped = FeedForward_GLM_Wrapped(4096, 32, i)\n",
    "    if i == 27:\n",
    "        copy_feedforward(transformer_layer, None, ff_wrapped)\n",
    "        ff_wrapped.layernorm_out = glm.condgen.transformer.final_layernorm.float()\n",
    "    else:\n",
    "        copy_feedforward(transformer_layer, raw_glm_layers[i + 1].float(), ff_wrapped)\n",
    "    ff_wrapped.requires_grad_(False)\n",
    "    ffs.append(ff_wrapped.cuda())\n",
    "\n",
    "word_embedding = glm.condgen.transformer.word_embeddings.weight.float().cuda()\n",
    "lm_head = glm.condgen.lm_head.float().cuda()\n",
    "input_layernorm = raw_glm_layers[0].input_layernorm.float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n0.space.attentions = attentions\n",
    "n1.space.attentions = n2.space.attentions = attentions_public\n",
    "n0.space.ffs = n1.space.ffs = ffs\n",
    "n0.space.word_embedding = word_embedding\n",
    "n0.space.input_layernorm = input_layernorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from split_llm.glm6b.secure_inference import GLM_Protocol\n",
    "\n",
    "protocol0 = GLM_Protocol(n0, Node.from_remote_name(\"n1\"), Node.from_remote_name(\"n2\"), 10, 500, device=\"cuda\")\n",
    "protocol1 = GLM_Protocol(Node.from_remote_name(\"n0\"), n1, Node.from_remote_name(\"n2\"), 10, 500, device=\"cuda\")\n",
    "protocol2 = GLM_Protocol(Node.from_remote_name(\"n0\"), Node.from_remote_name(\"n1\"), n2, 10, 500, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start prepare...\n",
      "Prepare stopped in 1.93e+03s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Start prepare...\")\n",
    "start_time = time.time()\n",
    "prepare_th1 = threading.Thread(target=protocol1.prepare)\n",
    "prepare_th2 = threading.Thread(target=protocol2.prepare)\n",
    "prepare_th1.start()\n",
    "prepare_th2.start()\n",
    "protocol0.prepare()\n",
    "prepare_th1.join()\n",
    "prepare_th2.join()\n",
    "print(f\"Prepare stopped in {time.time() - start_time:.3}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 130528])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:717: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608935911/work/torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  tensor = as_tensor(value)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def get_input_tensor(query: str):\n",
    "    input_ids, _, _ = glm.get_tokenization(query)\n",
    "    input_ids = input_ids[0]\n",
    "    input_selector = torch.zeros(len(input_ids), glm.n_tokens)\n",
    "    for i in range(len(input_ids)):\n",
    "        input_selector[i, input_ids[i]] = 1\n",
    "    return input_selector\n",
    "\n",
    "input_tensor = get_input_tensor(\"Hello\")\n",
    "print(input_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteratively_generate(query: str, length: int):\n",
    "    input_tensor = get_input_tensor(query).cuda()\n",
    "\n",
    "    generation_start_tensor = input_tensor[-1:]\n",
    "    input_tensor = input_tensor[:-1, :]\n",
    "    generated_ids = []\n",
    "    for i in range(length + 1):\n",
    "        print(\"Start offline execute...\")\n",
    "        start_time = time.time()\n",
    "        offline_th1 = threading.Thread(target=protocol1.offline_execute, args=(10,))\n",
    "        offline_th2 = threading.Thread(target=protocol2.offline_execute, args=(10,))\n",
    "        offline_th1.start()\n",
    "        offline_th2.start()\n",
    "        protocol0.offline_execute(10)\n",
    "        offline_th1.join()\n",
    "        offline_th2.join()\n",
    "        n1.storage[f\"{protocol1.name}:x\"] = input_tensor\n",
    "\n",
    "        print(f\"Offline execution stopped in {time.time() - start_time:.3}s.\")\n",
    "        print(\"Start online execute...\")\n",
    "        start_time = time.time()\n",
    "        online_th1 = threading.Thread(target=protocol1.online_execute)\n",
    "        online_th2 = threading.Thread(target=protocol2.online_execute)\n",
    "        online_th1.start()\n",
    "        online_th2.start()\n",
    "        protocol0.online_execute()\n",
    "        online_th1.join()\n",
    "        online_th2.join()\n",
    "        print(f\"Online execution stopped in {time.time() - start_time:.3}s.\")\n",
    "\n",
    "        if generation_start_tensor is None:\n",
    "            next_id = n1.storage[f\"{protocol1.name}:z\"][0]\n",
    "            generated_ids.append(next_id)\n",
    "            print(glm.decode(generated_ids[-1]), end=' ')\n",
    "            if next_id == glm.condgen.config.eos_token_id:\n",
    "                break\n",
    "            input_tensor = torch.zeros([1, glm.n_tokens]).cuda()\n",
    "            input_tensor[0, next_id] = 1\n",
    "        else:\n",
    "            input_tensor = generation_start_tensor\n",
    "            generation_start_tensor = None\n",
    "\n",
    "        print(glm.decode(generated_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-18 (offline_execute):\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/llm/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/llm/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 761, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/root/miniconda3/envs/llm/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/autodl-tmp/PermLLM/split_llm/glm6b/secure_inference.py\", line 948, in offline_execute\n",
      "    layer_protocol.offline_execute(next_length)\n",
      "  File \"/root/autodl-tmp/PermLLM/split_llm/glm6b/secure_inference.py\", line 634, in offline_execute\n",
      "    self.attn_protocol.offline_execute(next_length)\n",
      "  File \"/root/autodl-tmp/PermLLM/split_llm/glm6b/secure_inference.py\", line 164, in offline_execute\n",
      "    self.dot_product_protocol.offline_execute([next_length, 1, GLMConfig.n_heads, GLMConfig.head_dim], [next_length, self.total_length, 1, GLMConfig.n_heads], next_length)\n",
      "  File \"/root/autodl-tmp/PermLLM/split_llm/protocols/ss_mul_with_memory.py\", line 82, in offline_execute\n",
      "    w = self.f_mul(u, v)\n",
      "  File \"/root/autodl-tmp/PermLLM/split_llm/glm6b/secure_inference.py\", line 112, in <lambda>\n",
      "    (lambda k, q: local_node.space.attentions[layer].generate_logit_scores(q, k)),\n",
      "AttributeError: 'types.SimpleNamespace' object has no attribute 'attentions'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start offline execute...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43miteratively_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTell me about Trump\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m, in \u001b[0;36miteratively_generate\u001b[0;34m(query, length)\u001b[0m\n\u001b[1;32m     12\u001b[0m offline_th1\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     13\u001b[0m offline_th2\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m---> 14\u001b[0m \u001b[43mprotocol0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moffline_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m offline_th1\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[1;32m     16\u001b[0m offline_th2\u001b[38;5;241m.\u001b[39mjoin()\n",
      "File \u001b[0;32m~/autodl-tmp/PermLLM/split_llm/glm6b/secure_inference.py:948\u001b[0m, in \u001b[0;36mGLM_Protocol.offline_execute\u001b[0;34m(self, next_length)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_retrieval_protocol\u001b[38;5;241m.\u001b[39moffline_execute(next_length)\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_protocol \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_protocols:\n\u001b[0;32m--> 948\u001b[0m     \u001b[43mlayer_protocol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moffline_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_protocol\u001b[38;5;241m.\u001b[39moffline_execute()\n",
      "File \u001b[0;32m~/autodl-tmp/PermLLM/split_llm/glm6b/secure_inference.py:634\u001b[0m, in \u001b[0;36mGLM_TransformerLayerProtocol.offline_execute\u001b[0;34m(self, next_length)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moffline_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, next_length: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 634\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_protocol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moffline_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff_protocol\u001b[38;5;241m.\u001b[39moffline_execute(next_length)\n",
      "File \u001b[0;32m~/autodl-tmp/PermLLM/split_llm/glm6b/secure_inference.py:164\u001b[0m, in \u001b[0;36mGLM_AttentionProtocol.offline_execute\u001b[0;34m(self, next_length)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_lengths\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, next_length)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqkv_mul_protocol\u001b[38;5;241m.\u001b[39moffline_execute([next_length, \u001b[38;5;241m1\u001b[39m, GLMConfig\u001b[38;5;241m.\u001b[39mmodel_dim], [next_length, \u001b[38;5;241m1\u001b[39m, GLMConfig\u001b[38;5;241m.\u001b[39mmodel_dim \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot_product_protocol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moffline_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnext_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGLMConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGLMConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_dim\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mnext_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGLMConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_heads\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn0\u001b[38;5;241m.\u001b[39mlocal():\n\u001b[1;32m    167\u001b[0m     perm_key \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m30\u001b[39m)\n",
      "File \u001b[0;32m~/autodl-tmp/PermLLM/split_llm/protocols/ss_mul_with_memory.py:95\u001b[0m, in \u001b[0;36mSS_Mul__AppendingX.offline_execute\u001b[0;34m(self, y_shape, z_shape, append_size)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# In node_0\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_0\u001b[38;5;241m.\u001b[39mlocal():\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_and_enqueue\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m:beaver_v0, w0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# In node_1\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_1\u001b[38;5;241m.\u001b[39mlocal():\n",
      "File \u001b[0;32m~/autodl-tmp/PermLLM/split_llm/common/communication.py:113\u001b[0m, in \u001b[0;36mNode.fetch_and_enqueue\u001b[0;34m(self, from_role, header)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m header \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage[header] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage[header]\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrom_role\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/autodl-tmp/PermLLM/split_llm/common/communication.py:105\u001b[0m, in \u001b[0;36mNode.fetch\u001b[0;34m(self, from_role, header)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, from_role: \u001b[38;5;28mstr\u001b[39m, header: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_role\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/autodl-tmp/PermLLM/split_llm/common/real_communication.py:77\u001b[0m, in \u001b[0;36mRealCommunication.fetch\u001b[0;34m(self, to_role, from_role, header)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, to_role: \u001b[38;5;28mstr\u001b[39m, from_role: \u001b[38;5;28mstr\u001b[39m, header: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 77\u001b[0m     received_header, wrapped_obj \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_server_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mto_role\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrom_role\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m received_header \u001b[38;5;241m!=\u001b[39m header:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected header \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreceived_header\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheader\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/autodl-tmp/PermLLM/simple_socket/zf_socket.py:166\u001b[0m, in \u001b[0;36mSocketServer.recv_from\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SocketException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPeer name \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m dose not exist or not connected yet\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m name)\n\u001b[1;32m    165\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mother_recv_sockets[name]\n\u001b[0;32m--> 166\u001b[0m content \u001b[38;5;241m=\u001b[39m \u001b[43mread_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraffic_counter_from[name] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(content) \u001b[38;5;241m+\u001b[39m SocketConfig\u001b[38;5;241m.\u001b[39mlen_header \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(SocketConfig\u001b[38;5;241m.\u001b[39mstart_flag)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content\n",
      "File \u001b[0;32m~/autodl-tmp/PermLLM/simple_socket/zf_socket.py:24\u001b[0m, in \u001b[0;36mread_socket\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_socket\u001b[39m(s: socket\u001b[38;5;241m.\u001b[39msocket) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbytes\u001b[39m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m         flag \u001b[38;5;241m=\u001b[39m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSocketConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_flag\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m flag \u001b[38;5;241m!=\u001b[39m SocketConfig\u001b[38;5;241m.\u001b[39mstart_flag:\n\u001b[1;32m     26\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m SocketException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProtocol not match\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iteratively_generate(\"Tell me about Trump\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
