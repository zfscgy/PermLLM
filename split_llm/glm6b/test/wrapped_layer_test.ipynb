{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:10<00:00,  1.27s/it]\n"
     ]
    }
   ],
   "source": [
    "from llm_bases.chatglm6b import ChatGML6B\n",
    "glm = ChatGML6B()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "try:\n",
    "    del sys.modules[\"split_llm.glm6b.wrapped_layer\"]\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from split_llm.glm6b.wrapped_layer import Attention_GLM_Wrapped, FeedForward_GLM_Wrapped, copy_attention, copy_feedforward\n",
    "from split_llm.common.torch_utils import relative_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_layer = glm.condgen.transformer.layers[0].float()\n",
    "attn_wrapped = Attention_GLM_Wrapped(4096, 32, 0)\n",
    "feedforward_wrapped = FeedForward_GLM_Wrapped(4096, 32, 0)\n",
    "copy_attention(transformer_layer, attn_wrapped)\n",
    "copy_feedforward(transformer_layer, None, feedforward_wrapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from split_llm.glm6b.utils import generate_attention_mask, generate_position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = torch.rand([10, 1, 4096]).cuda()\n",
    "transformer_layer.cuda()\n",
    "attn_wrapped.cuda()\n",
    "feedforward_wrapped.cuda()\n",
    "position_ids = generate_position_ids(10, 10).cuda()\n",
    "attention_mask = generate_attention_mask(10, 10).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_raw = transformer_layer(xs, position_ids, attention_mask, 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the output by wrapped layers\n",
    "xs_normalized = transformer_layer.input_layernorm(xs)\n",
    "attn_out_raw = attn_wrapped(xs_normalized, position_ids)\n",
    "attn_out = attn_out_raw + (2 * 28) ** 0.5 * xs_normalized\n",
    "output_wrapped = feedforward_wrapped(attn_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error of transformer layer protocol: 0.00005\n"
     ]
    }
   ],
   "source": [
    "print(f\"Error of transformer layer protocol: {relative_error(output_wrapped, output_raw):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention input L2 norm 7.403954387942834\n",
      "Attention output L2 norm 0.4486609101295471\n"
     ]
    }
   ],
   "source": [
    "print(\"Attention input L2 norm\", torch.sqrt(torch.mean(torch.square(xs_normalized))).item() * (2 * 28) ** 0.5)\n",
    "print(\"Attention output L2 norm\", torch.sqrt(torch.mean(torch.square(attn_out_raw))).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the residual connected attention input has a larger scale ($\\approx 20\\times$) than the attention ouptut, which means that the adversary could be easy to distinguish between them?\n",
    "\n",
    "Specifically, suppose the attention is to be protected. \n",
    "\n",
    "Each time we feed $H, H'$ to attention but only use the true $H$ to produce the output $a$, then it could be easy to distinguish between $(H, H + a)$ and $(H, H' + a)$..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
