{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "from simple_socket.zf_socket import SocketServer\n",
    "from perm_llm.common.communication import Node\n",
    "from perm_llm.common.communication import Communication, Node, SimulatedCommunication\n",
    "from perm_llm.common.real_communication import RealCommunication\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "\n",
    "# Set up communication\n",
    "\n",
    "address_dict = {\n",
    "    \"127.0.0.1:3000\": \"n0\",\n",
    "    \"127.0.0.1:3001\": \"n1\",\n",
    "    \"127.0.0.1:3002\": \"n2\"\n",
    "}\n",
    "sock0 = SocketServer(\"127.0.0.1:3000\", address_dict, 20)\n",
    "sock1 = SocketServer(\"127.0.0.1:3001\", address_dict, 20)\n",
    "sock2 = SocketServer(\"127.0.0.1:3002\", address_dict, 20)\n",
    "\n",
    "time.sleep(1) # Wait the server to start listening\n",
    "\n",
    "sock0.connect_all()\n",
    "sock1.connect_all()\n",
    "sock2.connect_all()\n",
    "\n",
    "comm0 = RealCommunication({\"n0\": sock0}, tensor_device=device)\n",
    "comm1 = RealCommunication({\"n1\": sock1}, tensor_device=device)\n",
    "comm2 = RealCommunication({\"n2\": sock2}, tensor_device=device)\n",
    "n0 = Node(comm0, \"n0\")\n",
    "n1 = Node(comm1, \"n1\")\n",
    "n2 = Node(comm2, \"n2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:10<00:00,  1.25s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from perm_llm.glm6b.wrapped_layer import Attention_GLM_Wrapped, copy_attention, FeedForward_GLM_Wrapped, copy_feedforward\n",
    "from perm_llm.glm6b.utils import generate_position_ids\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from llm_bases.chatglm6b import ChatGML6B\n",
    "glm = ChatGML6B()\n",
    "\n",
    "from llm_bases.chatglm6b_official.modeling_chatglm import GLMBlock\n",
    "\n",
    "raw_glm_layers: List[GLMBlock] = glm.condgen.transformer.layers\n",
    "attentions: List[Attention_GLM_Wrapped] = []\n",
    "attentions_public: List[Attention_GLM_Wrapped] = []\n",
    "ffs: List[FeedForward_GLM_Wrapped] = []\n",
    "for i in range(28):\n",
    "    transformer_layer = raw_glm_layers[i].float()\n",
    "    \n",
    "    # The private attention layer\n",
    "    attn_wrapped = Attention_GLM_Wrapped(4096, 32, i)\n",
    "    copy_attention(transformer_layer, attn_wrapped)\n",
    "    attn_wrapped.requires_grad_(False)\n",
    "    attentions.append(attn_wrapped.cuda())\n",
    "\n",
    "    # The public attention layer\n",
    "    attn_wrapped_public = Attention_GLM_Wrapped(4096, 32, i)\n",
    "    attn_wrapped_public.qkv_weight = None\n",
    "    attn_wrapped_public.qkv_bias = None\n",
    "    attn_wrapped_public.attn_out_weight = None\n",
    "    attn_wrapped_public.attn_out_bias = None\n",
    "    attn_wrapped_public.positional_embedding = attn_wrapped.positional_embedding\n",
    "    attn_wrapped_public.requires_grad_(False)\n",
    "    attentions_public.append(attn_wrapped_public.cuda())\n",
    "\n",
    "    ff_wrapped = FeedForward_GLM_Wrapped(4096, 32, i)\n",
    "    if i == 27:\n",
    "        copy_feedforward(transformer_layer, None, ff_wrapped)\n",
    "        ff_wrapped.layernorm_out = glm.condgen.transformer.final_layernorm.float()\n",
    "    else:\n",
    "        copy_feedforward(transformer_layer, raw_glm_layers[i + 1].float(), ff_wrapped)\n",
    "    ff_wrapped.requires_grad_(False)\n",
    "    ffs.append(ff_wrapped.cuda())\n",
    "\n",
    "word_embedding = glm.condgen.transformer.word_embeddings.weight.float().cuda()\n",
    "lm_head = glm.condgen.lm_head.float().cuda()\n",
    "input_layernorm = raw_glm_layers[0].input_layernorm.float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'homomorphic_encryption.bfv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mperm_llm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mglm6b\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msecure_inference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GLM_Protocol\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mperm_llm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mglm6b\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msecure_inference_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_scale_dict\n",
      "File \u001b[0;32m~/autodl-tmp/PermLLM/perm_llm/glm6b/secure_inference.py:24\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mperm_llm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mglm6b\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_position_ids\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mperm_llm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m permute_2d_with_seed, permute_with_seed\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhomomorphic_encryption\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbfv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BFV\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtenseal\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mts\u001b[39;00m\n\u001b[1;32m     28\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSocket\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'homomorphic_encryption.bfv'"
     ]
    }
   ],
   "source": [
    "from perm_llm.glm6b.secure_inference import GLM_Protocol\n",
    "from perm_llm.glm6b.secure_inference_utils import generate_scale_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def get_input_tensor(query: str):\n",
    "    input_ids, _, _ = glm.get_tokenization(query)\n",
    "    input_ids = input_ids[0]\n",
    "    input_selector = torch.zeros(len(input_ids), glm.n_tokens)\n",
    "    for i in range(len(input_ids)):\n",
    "        input_selector[i, input_ids[i]] = 1\n",
    "    return input_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# n0_local1.storage[\"prediction/final_dense:z0\"] + n1_local1.storage[\"prediction/final_dense:z1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n0.space.attentions = attentions\n",
    "n1.space.attentions = n2.space.attentions = attentions_public\n",
    "n0.space.ffs = n1.space.ffs = ffs\n",
    "n0.space.word_embedding = word_embedding\n",
    "n0.space.input_layernorm = n1.space.input_layernorm = input_layernorm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_dict = generate_scale_dict(100)\n",
    "protocol0 = GLM_Protocol(n0, Node.from_remote_name(\"n1\"), Node.from_remote_name(\"n2\"), scale_dict, device=\"cuda\")\n",
    "protocol1 = GLM_Protocol(Node.from_remote_name(\"n0\"), n1, Node.from_remote_name(\"n2\"), scale_dict, device=\"cuda\")\n",
    "protocol2 = GLM_Protocol(Node.from_remote_name(\"n0\"), Node.from_remote_name(\"n1\"), n2, scale_dict, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start prepare...\n",
      "Prepare stopped in 86.4s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Start prepare...\")\n",
    "start_time = time.time()\n",
    "prepare_th1 = threading.Thread(target=protocol1.prepare)\n",
    "prepare_th2 = threading.Thread(target=protocol2.prepare)\n",
    "prepare_th1.start()\n",
    "prepare_th2.start()\n",
    "protocol0.prepare()\n",
    "prepare_th1.join()\n",
    "prepare_th2.join()\n",
    "print(f\"Prepare stopped in {time.time() - start_time:.3}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def offline(prompt_length: int, generation_length: int):\n",
    "    lengths = [prompt_length] + [1] * generation_length\n",
    "    for next_length in lengths:\n",
    "        print(\"Start offline execute...\")\n",
    "        start_time = time.time()\n",
    "        offline_th0 = threading.Thread(target=protocol0.offline_execute, args=(next_length,))\n",
    "        offline_th2 = threading.Thread(target=protocol2.offline_execute, args=(next_length,))\n",
    "        offline_th0.start()\n",
    "        offline_th2.start()\n",
    "        protocol1.offline_execute(next_length)\n",
    "        offline_th0.join()\n",
    "        offline_th2.join()\n",
    "        print(f\"Offline execution finished in {time.time() - start_time:.3}s.\")\n",
    "\n",
    "def iteratively_generate(query: str, length: int):\n",
    "    input_tensor = get_input_tensor(query).cuda()\n",
    "\n",
    "    generation_start_tensor = input_tensor[-1:]\n",
    "    input_tensor = input_tensor[:-1, :]\n",
    "    generated_ids = []\n",
    "    for i in range(length + 1):\n",
    "        n1.storage[f\"{protocol1.name}:x\"] = input_tensor\n",
    "        print(\"Start online execute...\")\n",
    "        start_time = time.time()\n",
    "        online_th0 = threading.Thread(target=protocol0.online_execute)\n",
    "        online_th2 = threading.Thread(target=protocol2.online_execute)\n",
    "        online_th0.start()\n",
    "        online_th2.start()\n",
    "        protocol1.online_execute()\n",
    "        online_th0.join()\n",
    "        online_th2.join()\n",
    "        print(f\"Online execution finished in {time.time() - start_time:.3}s.\")\n",
    "\n",
    "        if generation_start_tensor is None:\n",
    "            next_id = n1.storage[f\"{protocol1.name}:z\"][0]\n",
    "            # print(next_id)\n",
    "            generated_ids.append(next_id)\n",
    "            if next_id == glm.condgen.config.eos_token_id:\n",
    "                break\n",
    "            input_tensor = torch.zeros([1, glm.n_tokens]).cuda()\n",
    "            input_tensor[0, next_id] = 1\n",
    "        else:\n",
    "            next_id = n1.storage[f\"{protocol1.name}:z\"][0]\n",
    "            # print(next_id)\n",
    "            input_tensor = generation_start_tensor\n",
    "            generation_start_tensor = None\n",
    "\n",
    "        print(glm.decode(generated_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm0.simulate_network(None, None)\n",
    "comm1.simulate_network(None, None)\n",
    "comm2.simulate_network(None, None)\n",
    "\n",
    "try:\n",
    "    protocol0.reset()\n",
    "    protocol1.reset()\n",
    "    protocol2.reset()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start offline execute...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:717: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608935911/work/torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  tensor = as_tensor(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offline execution finished in 1.06s.\n",
      "Start offline execute...\n",
      "Offline execution finished in 0.318s.\n",
      "Start offline execute...\n",
      "Offline execution finished in 0.406s.\n",
      "Start offline execute...\n",
      "Offline execution finished in 0.316s.\n",
      "Start offline execute...\n",
      "Offline execution finished in 0.317s.\n",
      "Start offline execute...\n",
      "Offline execution finished in 0.313s.\n",
      "Start offline execute...\n",
      "Offline execution finished in 0.316s.\n",
      "Start offline execute...\n",
      "Offline execution finished in 0.315s.\n",
      "Start offline execute...\n",
      "Offline execution finished in 0.315s.\n",
      "Start offline execute...\n",
      "Offline execution finished in 0.399s.\n",
      "Start offline execute...\n",
      "Offline execution finished in 0.319s.\n",
      "Start offline execute...\n",
      "Offline execution finished in 0.319s.\n",
      "Start offline execute...\n",
      "Offline execution finished in 0.317s.\n",
      "Start offline execute...\n",
      "Offline execution finished in 0.322s.\n",
      "Start offline execute...\n",
      "Offline execution finished in 0.324s.\n",
      "Start offline execute...\n",
      "Offline execution finished in 0.422s.\n",
      "Start offline execute...\n",
      "Offline execution finished in 0.324s.\n",
      "Start offline execute...\n",
      "Offline execution finished in 0.324s.\n",
      "Start offline execute...\n",
      "Offline execution finished in 0.319s.\n",
      "Start offline execute...\n",
      "Offline execution finished in 0.321s.\n",
      "Start offline execute...\n",
      "Offline execution finished in 0.319s.\n",
      "Start offline execute...\n",
      "Offline execution finished in 0.422s.\n",
      "Start offline execute...\n",
      "Offline execution finished in 0.324s.\n",
      "Start offline execute...\n",
      "Offline execution finished in 0.321s.\n",
      "Start offline execute...\n",
      "Offline execution finished in 0.321s.\n",
      "Start offline execute...\n",
      "Offline execution finished in 0.321s.\n",
      "Start offline execute...\n",
      "Offline execution finished in 0.318s.\n",
      "Start offline execute...\n",
      "Offline execution finished in 0.319s.\n",
      "Start offline execute...\n",
      "Offline execution finished in 0.406s.\n",
      "Start offline execute...\n",
      "Offline execution finished in 0.319s.\n",
      "Start offline execute...\n",
      "Offline execution finished in 0.426s.\n"
     ]
    }
   ],
   "source": [
    "query = \"How many stars are in the sky?\"\n",
    "generation_length = 30\n",
    "offline(len(glm.get_tokenization(query)[0][0]) - 1, generation_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start online execute...\n",
      "Online execution finished in 2.31s.\n",
      "\n",
      "Start online execute...\n",
      "Online execution finished in 1.18s.\n",
      "It\n",
      "Start online execute...\n",
      "Online execution finished in 1.18s.\n",
      "It is\n",
      "Start online execute...\n",
      "Online execution finished in 1.18s.\n",
      "It is difficult\n",
      "Start online execute...\n",
      "Online execution finished in 1.18s.\n",
      "It is difficult to\n",
      "Start online execute...\n",
      "Online execution finished in 1.18s.\n",
      "It is difficult to give\n",
      "Start online execute...\n",
      "Online execution finished in 1.18s.\n",
      "It is difficult to give an\n",
      "Start online execute...\n",
      "Online execution finished in 1.18s.\n",
      "It is difficult to give an exact\n",
      "Start online execute...\n",
      "Online execution finished in 1.18s.\n",
      "It is difficult to give an exact number\n",
      "Start online execute...\n",
      "Online execution finished in 1.18s.\n",
      "It is difficult to give an exact number of\n",
      "Start online execute...\n",
      "Online execution finished in 1.23s.\n",
      "It is difficult to give an exact number of stars\n",
      "Start online execute...\n",
      "Online execution finished in 1.19s.\n",
      "It is difficult to give an exact number of stars in\n",
      "Start online execute...\n",
      "Online execution finished in 1.18s.\n",
      "It is difficult to give an exact number of stars in the\n",
      "Start online execute...\n",
      "Online execution finished in 1.18s.\n",
      "It is difficult to give an exact number of stars in the sky\n",
      "Start online execute...\n",
      "Online execution finished in 1.18s.\n",
      "It is difficult to give an exact number of stars in the sky,\n",
      "Start online execute...\n",
      "Online execution finished in 1.18s.\n",
      "It is difficult to give an exact number of stars in the sky, as\n",
      "Start online execute...\n",
      "Online execution finished in 1.18s.\n",
      "It is difficult to give an exact number of stars in the sky, as the\n",
      "Start online execute...\n",
      "Online execution finished in 1.18s.\n",
      "It is difficult to give an exact number of stars in the sky, as the number\n",
      "Start online execute...\n",
      "Online execution finished in 1.18s.\n",
      "It is difficult to give an exact number of stars in the sky, as the number of\n",
      "Start online execute...\n",
      "Online execution finished in 1.18s.\n",
      "It is difficult to give an exact number of stars in the sky, as the number of stars\n",
      "Start online execute...\n",
      "Online execution finished in 1.18s.\n",
      "It is difficult to give an exact number of stars in the sky, as the number of stars in\n",
      "Start online execute...\n",
      "Online execution finished in 1.18s.\n",
      "It is difficult to give an exact number of stars in the sky, as the number of stars in the\n",
      "Start online execute...\n",
      "Online execution finished in 1.18s.\n",
      "It is difficult to give an exact number of stars in the sky, as the number of stars in the universe\n",
      "Start online execute...\n",
      "Online execution finished in 1.18s.\n",
      "It is difficult to give an exact number of stars in the sky, as the number of stars in the universe is\n",
      "Start online execute...\n",
      "Online execution finished in 1.21s.\n",
      "It is difficult to give an exact number of stars in the sky, as the number of stars in the universe is constantly\n",
      "Start online execute...\n",
      "Online execution finished in 1.18s.\n",
      "It is difficult to give an exact number of stars in the sky, as the number of stars in the universe is constantly changing\n",
      "Start online execute...\n",
      "Online execution finished in 1.19s.\n",
      "It is difficult to give an exact number of stars in the sky, as the number of stars in the universe is constantly changing.\n",
      "Start online execute...\n",
      "Online execution finished in 1.2s.\n",
      "It is difficult to give an exact number of stars in the sky, as the number of stars in the universe is constantly changing. However\n",
      "Start online execute...\n",
      "Online execution finished in 1.19s.\n",
      "It is difficult to give an exact number of stars in the sky, as the number of stars in the universe is constantly changing. However,\n",
      "Start online execute...\n",
      "Online execution finished in 1.18s.\n",
      "It is difficult to give an exact number of stars in the sky, as the number of stars in the universe is constantly changing. However, the\n",
      "Start online execute...\n",
      "Online execution finished in 1.18s.\n",
      "It is difficult to give an exact number of stars in the sky, as the number of stars in the universe is constantly changing. However, the total\n"
     ]
    }
   ],
   "source": [
    "comm0.simulate_network(10, 1000)\n",
    "comm1.simulate_network(10, 1000)\n",
    "comm2.simulate_network(10, 1000)\n",
    "iteratively_generate(query, generation_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
