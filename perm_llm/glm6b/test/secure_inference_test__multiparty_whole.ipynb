{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "from simple_socket.zf_socket import SocketServer\n",
    "from perm_llm.common.communication import Node\n",
    "from perm_llm.common.communication import Communication, Node, SimulatedCommunication\n",
    "from perm_llm.common.real_communication import RealCommunication\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "\n",
    "# Set up communication\n",
    "\n",
    "address_dict = {\n",
    "    \"127.0.0.1:3000\": \"n0\",\n",
    "    \"127.0.0.1:3001\": \"n1\",\n",
    "    \"127.0.0.1:3002\": \"n2\"\n",
    "}\n",
    "sock0 = SocketServer(\"127.0.0.1:3000\", address_dict, 20)\n",
    "sock1 = SocketServer(\"127.0.0.1:3001\", address_dict, 20)\n",
    "sock2 = SocketServer(\"127.0.0.1:3002\", address_dict, 20)\n",
    "\n",
    "time.sleep(1) # Wait the server to start listening\n",
    "\n",
    "sock0.connect_all()\n",
    "sock1.connect_all()\n",
    "sock2.connect_all()\n",
    "\n",
    "comm0 = RealCommunication({\"n0\": sock0}, tensor_device=device)\n",
    "comm1 = RealCommunication({\"n1\": sock1}, tensor_device=device)\n",
    "comm2 = RealCommunication({\"n2\": sock2}, tensor_device=device)\n",
    "n0 = Node(comm0, \"n0\")\n",
    "n1 = Node(comm1, \"n1\")\n",
    "n2 = Node(comm2, \"n2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from perm_llm.glm6b.wrapped_layer import Attention_GLM_Wrapped, copy_attention, FeedForward_GLM_Wrapped, copy_feedforward\n",
    "from perm_llm.glm6b.utils import generate_position_ids\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from llm_bases.chatglm6b import ChatGML6B\n",
    "glm = ChatGML6B()\n",
    "\n",
    "from llm_bases.chatglm6b_official.modeling_chatglm import GLMBlock\n",
    "\n",
    "raw_glm_layers: List[GLMBlock] = glm.condgen.transformer.layers\n",
    "attentions: List[Attention_GLM_Wrapped] = []\n",
    "attentions_public: List[Attention_GLM_Wrapped] = []\n",
    "ffs: List[FeedForward_GLM_Wrapped] = []\n",
    "for i in range(28):\n",
    "    transformer_layer = raw_glm_layers[i].float()\n",
    "    \n",
    "    # The private attention layer\n",
    "    attn_wrapped = Attention_GLM_Wrapped(4096, 32, i)\n",
    "    copy_attention(transformer_layer, attn_wrapped)\n",
    "    attn_wrapped.requires_grad_(False)\n",
    "    attentions.append(attn_wrapped.cuda())\n",
    "\n",
    "    # The public attention layer\n",
    "    attn_wrapped_public = Attention_GLM_Wrapped(4096, 32, i)\n",
    "    attn_wrapped_public.qkv_weight = None\n",
    "    attn_wrapped_public.qkv_bias = None\n",
    "    attn_wrapped_public.attn_out_weight = None\n",
    "    attn_wrapped_public.attn_out_bias = None\n",
    "    attn_wrapped_public.positional_embedding = attn_wrapped.positional_embedding\n",
    "    attn_wrapped_public.requires_grad_(False)\n",
    "    attentions_public.append(attn_wrapped_public.cuda())\n",
    "\n",
    "    ff_wrapped = FeedForward_GLM_Wrapped(4096, 32, i)\n",
    "    if i == 27:\n",
    "        copy_feedforward(transformer_layer, None, ff_wrapped)\n",
    "        ff_wrapped.layernorm_out = glm.condgen.transformer.final_layernorm.float()\n",
    "    else:\n",
    "        copy_feedforward(transformer_layer, raw_glm_layers[i + 1].float(), ff_wrapped)\n",
    "    ff_wrapped.requires_grad_(False)\n",
    "    ffs.append(ff_wrapped.cuda())\n",
    "\n",
    "word_embedding = glm.condgen.transformer.word_embeddings.weight.float().cuda()\n",
    "lm_head = glm.condgen.lm_head.float().cuda()\n",
    "input_layernorm = raw_glm_layers[0].input_layernorm.float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from perm_llm.glm6b.secure_inference import GLM_Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def get_input_tensor(query: str):\n",
    "    input_ids, _, _ = glm.get_tokenization(query)\n",
    "    input_ids = input_ids[0]\n",
    "    input_selector = torch.zeros(len(input_ids), glm.n_tokens)\n",
    "    for i in range(len(input_ids)):\n",
    "        input_selector[i, input_ids[i]] = 1\n",
    "    return input_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# n0_local1.storage[\"prediction/final_dense:z0\"] + n1_local1.storage[\"prediction/final_dense:z1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n0.space.attentions = attentions\n",
    "n1.space.attentions = n2.space.attentions = attentions_public\n",
    "n0.space.ffs = n1.space.ffs = ffs\n",
    "n0.space.word_embedding = word_embedding\n",
    "n0.space.input_layernorm = n1.space.input_layernorm = input_layernorm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol0 = GLM_Protocol(n0, Node.from_remote_name(\"n1\"), Node.from_remote_name(\"n2\"), 1, 100, device=\"cuda\")\n",
    "protocol1 = GLM_Protocol(Node.from_remote_name(\"n0\"), n1, Node.from_remote_name(\"n2\"), 1, 100, device=\"cuda\")\n",
    "protocol2 = GLM_Protocol(Node.from_remote_name(\"n0\"), Node.from_remote_name(\"n1\"), n2, 1, 100, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start prepare...\")\n",
    "start_time = time.time()\n",
    "prepare_th1 = threading.Thread(target=protocol1.prepare)\n",
    "prepare_th2 = threading.Thread(target=protocol2.prepare)\n",
    "prepare_th1.start()\n",
    "prepare_th2.start()\n",
    "protocol0.prepare()\n",
    "prepare_th1.join()\n",
    "prepare_th2.join()\n",
    "print(f\"Prepare stopped in {time.time() - start_time:.3}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def offline(prompt_length: int, generation_length: int):\n",
    "    lengths = [prompt_length] + [1] * generation_length\n",
    "    for next_length in lengths:\n",
    "        print(\"Start offline execute...\")\n",
    "        start_time = time.time()\n",
    "        offline_th0 = threading.Thread(target=protocol0.offline_execute, args=(next_length,))\n",
    "        offline_th2 = threading.Thread(target=protocol2.offline_execute, args=(next_length,))\n",
    "        offline_th0.start()\n",
    "        offline_th2.start()\n",
    "        protocol1.offline_execute(next_length)\n",
    "        offline_th0.join()\n",
    "        offline_th2.join()\n",
    "        print(f\"Offline execution finished in {time.time() - start_time:.3}s.\")\n",
    "\n",
    "def iteratively_generate(query: str, length: int):\n",
    "    input_tensor = get_input_tensor(query).cuda()\n",
    "\n",
    "    generation_start_tensor = input_tensor[-1:]\n",
    "    input_tensor = input_tensor[:-1, :]\n",
    "    generated_ids = []\n",
    "    for i in range(length + 1):\n",
    "        n1.storage[f\"{protocol1.name}:x\"] = input_tensor\n",
    "        print(\"Start online execute...\")\n",
    "        start_time = time.time()\n",
    "        online_th0 = threading.Thread(target=protocol0.online_execute)\n",
    "        online_th2 = threading.Thread(target=protocol2.online_execute)\n",
    "        online_th0.start()\n",
    "        online_th2.start()\n",
    "        protocol1.online_execute()\n",
    "        online_th0.join()\n",
    "        online_th2.join()\n",
    "        print(f\"Online execution finished in {time.time() - start_time:.3}s.\")\n",
    "\n",
    "        if generation_start_tensor is None:\n",
    "            next_id = n1.storage[f\"{protocol1.name}:z\"][0]\n",
    "            # print(next_id)\n",
    "            generated_ids.append(next_id)\n",
    "            if next_id == glm.condgen.config.eos_token_id:\n",
    "                break\n",
    "            input_tensor = torch.zeros([1, glm.n_tokens]).cuda()\n",
    "            input_tensor[0, next_id] = 1\n",
    "        else:\n",
    "            next_id = n1.storage[f\"{protocol1.name}:z\"][0]\n",
    "            # print(next_id)\n",
    "            input_tensor = generation_start_tensor\n",
    "            generation_start_tensor = None\n",
    "\n",
    "        print(glm.decode(generated_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How many stars are in the sky?\"\n",
    "generation_length = 30\n",
    "offline(len(glm.get_tokenization(query)[0][0]) - 1, generation_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteratively_generate(query, generation_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
