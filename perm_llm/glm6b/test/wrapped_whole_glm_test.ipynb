{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.54s/it]\n"
     ]
    }
   ],
   "source": [
    "from llm_bases.chatglm6b import ChatGML6B\n",
    "glm = ChatGML6B()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-8.4076e-03, -9.3689e-03, -5.4436e-03,  ..., -6.4545e-03,\n",
      "          1.6998e-02,  1.1108e-02],\n",
      "        [-1.0071e-02,  5.8022e-03,  4.8018e-04,  ..., -4.2701e-04,\n",
      "          1.0252e-03, -1.6556e-03],\n",
      "        [ 1.9424e-02,  6.3477e-03,  2.4933e-02,  ...,  5.7297e-03,\n",
      "          1.2512e-02,  9.4147e-03],\n",
      "        ...,\n",
      "        [-1.0078e-02,  3.0041e-03,  2.4376e-03,  ..., -4.7684e-06,\n",
      "          1.5430e-03,  1.1053e-03],\n",
      "        [-9.9945e-03,  4.4479e-03,  6.2141e-03,  ...,  1.7560e-04,\n",
      "          1.4286e-03, -1.1883e-03],\n",
      "        [-9.3536e-03,  1.7376e-03,  5.7373e-03,  ..., -1.0910e-03,\n",
      "          4.3945e-03, -1.2541e-03]])\n"
     ]
    }
   ],
   "source": [
    "print(glm.condgen.lm_head.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from perm_llm.glm6b.wrapped_layer import Attention_GLM_Wrapped, FeedForward_GLM_Wrapped, copy_attention, copy_feedforward\n",
    "from perm_llm.common.torch_utils import relative_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_glm_layers = glm.condgen.transformer.layers\n",
    "attentions: List[Attention_GLM_Wrapped] = []\n",
    "ffs: List[FeedForward_GLM_Wrapped] = []\n",
    "for i in range(28):\n",
    "    transformer_layer = raw_glm_layers[i].float()\n",
    "    attn_wrapped = Attention_GLM_Wrapped(4096, 32, i)\n",
    "    copy_attention(transformer_layer, attn_wrapped)\n",
    "    attentions.append(attn_wrapped.cuda())\n",
    "\n",
    "    ff_wrapped = FeedForward_GLM_Wrapped(4096, 32, i)\n",
    "    if i == 27:\n",
    "        copy_feedforward(transformer_layer, None, ff_wrapped)\n",
    "        ff_wrapped.layernorm_out = glm.condgen.transformer.final_layernorm.float().cuda()\n",
    "    else:\n",
    "        copy_feedforward(transformer_layer, raw_glm_layers[i + 1].float(), ff_wrapped)\n",
    "    ffs.append(ff_wrapped.cuda())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from perm_llm.glm6b.utils import generate_attention_mask, generate_position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Scale:   0.0109\n"
     ]
    }
   ],
   "source": [
    "# Convert the GLM model weights (embeddings at the beginning and the end) into CUDA-float\n",
    "\n",
    "glm.condgen.transformer.word_embeddings.float().cuda()\n",
    "\n",
    "print(f\"Embedding Scale: {glm.condgen.transformer.word_embeddings.weight.std().item():8.4f}\")\n",
    "\n",
    "glm.condgen.lm_head.float().cuda()\n",
    "\n",
    "layernorm0 = glm.condgen.transformer.layers[0].input_layernorm.float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_hidden_scales(query: str, generation_length: int):\n",
    "    input_ids, position_ids, attention_masks = glm.get_tokenization(query)\n",
    "    \n",
    "    \n",
    "    original_length = len(input_ids[0])\n",
    "    input_ids = input_ids.cuda()\n",
    "    start_id = input_ids[0, -1]\n",
    "    input_ids = input_ids[:, :-1]\n",
    "    position_ids = position_ids.cuda()[:, :, :-1]\n",
    "    # The start_id has an attention mask\n",
    "\n",
    "    \n",
    "    all_scales = []\n",
    "    previous_k = dict()\n",
    "    previous_v = dict()\n",
    "\n",
    "    predicted_ids = []\n",
    "\n",
    "    for i in range(generation_length):\n",
    "        # print(f\"Generate {i}-th token\")\n",
    "        # Here the first layernorm is moved\n",
    "        current_scales = dict()\n",
    "\n",
    "        initial_state = glm.get_initial_state(input_ids)\n",
    "        current_scales[\"word_embedding\"] = initial_state.std().item()\n",
    "\n",
    "        h = layernorm0(initial_state)\n",
    "        \n",
    "        for j in range(28):\n",
    "            scales = dict()\n",
    "            # print(\"Layer\", j)\n",
    "            # Forward to the i-th attention layer    \n",
    "            scales[\"h_in\"] = h.std().item()\n",
    "\n",
    "            # Attention module\n",
    "            attn_layer: Attention_GLM_Wrapped = attentions[j]\n",
    "            scales[\"qkv_linear\"] = attn_layer.qkv_weight.std().item()\n",
    "\n",
    "            \n",
    "            q, k, v = attn_layer.generate_qkv(h, position_ids)\n",
    "\n",
    "            \n",
    "            if j not in previous_k:\n",
    "                previous_k[j] = k\n",
    "            else:\n",
    "                previous_k[j] = torch.cat([previous_k[j], k], dim=0)\n",
    "                k = previous_k[j]\n",
    "\n",
    "            \n",
    "            if j not in previous_v:\n",
    "                previous_v[j] = v\n",
    "            else:\n",
    "                previous_v[j] = torch.cat([previous_v[j], v], dim=0)\n",
    "                v = previous_v[j]\n",
    "\n",
    "\n",
    "            scales[\"v\"] = max(q.std().item(), k.std().item(), v.std().item())  # Record the scale of V\n",
    "\n",
    "            attn_scores = attn_layer.generate_logit_scores(q, k)\n",
    "            scales[\"attn_scores\"] = attn_scores.std().item()  # Record the scale since here the plaintext is used\n",
    "\n",
    "            softmax_scores = attn_layer.generate_softmax_scores(attn_scores)\n",
    "            weighted_v = attn_layer.generate_weighted_values(softmax_scores, v)\n",
    "\n",
    "            attn_out = weighted_v @ attn_layer.attn_out_weight.T + attn_layer.attn_out_bias\n",
    "            scales[\"attn_out_linear\"] = attn_layer.attn_out_weight.std().item()\n",
    "            scales[\"attn_out\"] = attn_out.std().item()\n",
    "\n",
    "            # Feedforward module\n",
    "            ff_layer: FeedForward_GLM_Wrapped = ffs[j]\n",
    "            \n",
    "            h_ff_in = attn_out + h * (2 * 28) ** 0.5\n",
    "            scales[\"ff_h_in\"] = h_ff_in.std().item()\n",
    "            h0 = ff_layer.layernorm_in(h_ff_in)\n",
    "\n",
    "            h1 = ff_layer.mlp_dense_in(h0)\n",
    "            scales[\"mlp_dense_in\"] = ff_layer.mlp_dense_in.weight.std().item()\n",
    "            \n",
    "            scales[\"mlp_hidden\"] = h1.std().item()\n",
    "            h2 = F.gelu(h1)\n",
    "\n",
    "            #  h2 = gelu_openai(h1)\n",
    "            #  Those two gelu implementations do not have significant difference\n",
    "            h3 = ff_layer.mlp_dense_out(h2)\n",
    "            scales[\"mlp_dense_out\"] = ff_layer.mlp_dense_out.weight.std().item()\n",
    "            scales[\"mlp_out\"] = h3.std().item()\n",
    "\n",
    "            h4 = h3 + ff_layer.residual_coef * h0\n",
    "\n",
    "            scales[\"ff_out\"] = h4.std().item()\n",
    "            h5 = ff_layer.layernorm_out(h4)\n",
    "\n",
    "            h = h5\n",
    "\n",
    "            current_scales.update({f\"layer_{j}-{k}\": scales[k] for k in scales})\n",
    "\n",
    "        logits = glm.condgen.lm_head(h).permute(1, 0, 2).contiguous()[0, -1]\n",
    "        current_scales[\"logits\"] = logits.std().item()\n",
    "        # Get the logits on the next position\n",
    "        if start_id is not None:\n",
    "            next_id = start_id\n",
    "            start_id = None\n",
    "        else:\n",
    "            next_id = torch.argmax(logits)\n",
    "            predicted_ids.append(next_id.item())\n",
    "        # print(\"Next ID\", next_id)\n",
    "\n",
    "        if next_id == glm.condgen.generation_config.eos_token_id:\n",
    "            break\n",
    "        input_ids = torch.tensor([[next_id]]).cuda()  # Append the last id\n",
    "        position_ids = generate_position_ids(original_length, original_length + len(predicted_ids))[:, :, -1:].cuda()\n",
    "        # print(position_ids)\n",
    "        all_scales.append(current_scales)\n",
    "    \n",
    "    print(predicted_ids)\n",
    "    print(glm.decode(predicted_ids))\n",
    "    return pd.DataFrame(all_scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3347, 729, 107, 104, 896, 618, 6636, 172, 1814, 114, 100, 5, 16, 15, 257, 1053, 101, 100, 494, 608, 122, 1151, 5, 10, 8, 6, 5, 10, 8, 9, 25, 6, 103, 1151, 5, 10, 8, 6, 5, 10, 8, 10, 9, 7, 256, 116, 2258, 111, 1097, 5, 9, 16, 6, 5, 9, 18, 16, 21, 6, 105, 375, 875, 928, 6, 375, 875, 7, 729, 107, 104, 16492, 102, 644, 3890, 4770, 172, 132, 156, 958, 105, 687, 611, 37188, 102, 132, 156, 100, 1319, 101, 2971, 6603, 102, 1558, 3042, 7, 4, 4, 3663, 147, 15063, 6, 729, 116, 424, 108, 147, 6781, 3418, 6, 350, 147, 2256, 103, 975, 104, 1710, 111, 100, 289, 7, 159, 7, 11, 26291, 1683, 6, 147, 12544, 111, 5638, 6, 102, 147, 3980, 101, 100, 11983, 11, 9, 18, 15190, 7, 256, 154, 5549, 2971, 37391, 102, 5768, 6, 350, 2414, 12376, 102, 5273, 6, 611, 6252, 6, 102, 1251, 5663, 7, 4, 4, 1252, 2787, 1189, 6, 729, 2037, 103, 113, 104, 6781, 1854, 6, 4111, 2971, 1558, 3042, 102, 6603, 7, 256, 132, 156, 100, 1319, 101, 2971, 22016, 6, 350, 145, 822, 103, 100, 5, 10, 8, 10, 9, 1758, 7, 130005]\n",
      "Donald Trump is a former American politician who served as the 45th President of the United States from January 20, 2017, to January 20, 2021. He was born on June 14, 1946, in New York City, New York. Trump is a businessman and real estate developer who has been involved in various business ventures and has been the subject of numerous investigations and legal challenges.\n",
      "\n",
      "During his presidency, Trump was known for his controversial policies, including his efforts to build a wall on the U.S.-Mexico border, his stance on immigration, and his handling of the COVID-19 pandemic. He also faced numerous controversies and allegations, including sexual harassment and assault, business fraud, and political contributions.\n",
      "\n",
      "After leaving office, Trump continued to be a controversial figure, facing numerous legal challenges and investigations. He has been the subject of numerous lawsuits, including one related to the 2021 election.\n"
     ]
    }
   ],
   "source": [
    "scales_df = analyze_hidden_scales(\"Tell me about Trump\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            word_embedding   0.0110\n",
      "              layer_0-h_in   0.9433\n",
      "        layer_0-qkv_linear   0.0128\n",
      "                 layer_0-v   2.0570\n",
      "       layer_0-attn_scores   4.7131\n",
      "   layer_0-attn_out_linear   0.0126\n",
      "          layer_0-attn_out   4.6358\n",
      "           layer_0-ff_h_in   9.1073\n",
      "      layer_0-mlp_dense_in   0.0156\n",
      "        layer_0-mlp_hidden   1.3827\n",
      "     layer_0-mlp_dense_out   0.0155\n",
      "           layer_0-mlp_out   3.1483\n",
      "            layer_0-ff_out  10.2210\n",
      "              layer_1-h_in   1.0298\n",
      "        layer_1-qkv_linear   0.0138\n",
      "                 layer_1-v   1.8481\n",
      "       layer_1-attn_scores   4.4332\n",
      "   layer_1-attn_out_linear   0.0142\n",
      "          layer_1-attn_out   2.1595\n",
      "           layer_1-ff_h_in   8.5945\n",
      "      layer_1-mlp_dense_in   0.0166\n",
      "        layer_1-mlp_hidden   1.5104\n",
      "     layer_1-mlp_dense_out   0.0166\n",
      "           layer_1-mlp_out   2.0980\n",
      "            layer_1-ff_out   9.4040\n",
      "              layer_2-h_in   1.0057\n",
      "        layer_2-qkv_linear   0.0142\n",
      "                 layer_2-v   1.6114\n",
      "       layer_2-attn_scores   2.7455\n",
      "   layer_2-attn_out_linear   0.0144\n",
      "          layer_2-attn_out   1.3957\n",
      "           layer_2-ff_h_in   7.9599\n",
      "      layer_2-mlp_dense_in   0.0172\n",
      "        layer_2-mlp_hidden   1.5355\n",
      "     layer_2-mlp_dense_out   0.0172\n",
      "           layer_2-mlp_out   1.6428\n",
      "            layer_2-ff_out   8.8335\n",
      "              layer_3-h_in   1.0022\n",
      "        layer_3-qkv_linear   0.0144\n",
      "                 layer_3-v   1.5400\n",
      "       layer_3-attn_scores   2.6815\n",
      "   layer_3-attn_out_linear   0.0146\n",
      "          layer_3-attn_out   1.3863\n",
      "           layer_3-ff_h_in   7.9130\n",
      "      layer_3-mlp_dense_in   0.0176\n",
      "        layer_3-mlp_hidden   1.5958\n",
      "     layer_3-mlp_dense_out   0.0177\n",
      "           layer_3-mlp_out   1.3394\n",
      "            layer_3-ff_out   8.4455\n",
      "              layer_4-h_in   1.0121\n",
      "        layer_4-qkv_linear   0.0147\n",
      "                 layer_4-v   1.5917\n",
      "       layer_4-attn_scores   2.7808\n",
      "   layer_4-attn_out_linear   0.0146\n",
      "          layer_4-attn_out   1.2000\n",
      "           layer_4-ff_h_in   7.9229\n",
      "      layer_4-mlp_dense_in   0.0179\n",
      "        layer_4-mlp_hidden   1.6486\n",
      "     layer_4-mlp_dense_out   0.0180\n",
      "           layer_4-mlp_out   1.1175\n",
      "            layer_4-ff_out   8.3027\n",
      "              layer_5-h_in   1.0162\n",
      "        layer_5-qkv_linear   0.0148\n",
      "                 layer_5-v   1.5567\n",
      "       layer_5-attn_scores   3.0783\n",
      "   layer_5-attn_out_linear   0.0146\n",
      "          layer_5-attn_out   1.1537\n",
      "           layer_5-ff_h_in   7.9100\n",
      "      layer_5-mlp_dense_in   0.0182\n",
      "        layer_5-mlp_hidden   1.7014\n",
      "     layer_5-mlp_dense_out   0.0180\n",
      "           layer_5-mlp_out   1.1091\n",
      "            layer_5-ff_out   8.3335\n",
      "              layer_6-h_in   1.0013\n",
      "        layer_6-qkv_linear   0.0148\n",
      "                 layer_6-v   1.5433\n",
      "       layer_6-attn_scores   2.9336\n",
      "   layer_6-attn_out_linear   0.0146\n",
      "          layer_6-attn_out   1.0497\n",
      "           layer_6-ff_h_in   7.7700\n",
      "      layer_6-mlp_dense_in   0.0181\n",
      "        layer_6-mlp_hidden   1.6938\n",
      "     layer_6-mlp_dense_out   0.0175\n",
      "           layer_6-mlp_out   1.1579\n",
      "            layer_6-ff_out   8.3173\n",
      "              layer_7-h_in   0.9953\n",
      "        layer_7-qkv_linear   0.0149\n",
      "                 layer_7-v   1.6625\n",
      "       layer_7-attn_scores   4.2284\n",
      "   layer_7-attn_out_linear   0.0148\n",
      "          layer_7-attn_out   0.9978\n",
      "           layer_7-ff_h_in   7.6722\n",
      "      layer_7-mlp_dense_in   0.0179\n",
      "        layer_7-mlp_hidden   1.6329\n",
      "     layer_7-mlp_dense_out   0.0171\n",
      "           layer_7-mlp_out   1.1440\n",
      "            layer_7-ff_out   8.3431\n",
      "              layer_8-h_in   0.9872\n",
      "        layer_8-qkv_linear   0.0153\n",
      "                 layer_8-v   1.7945\n",
      "       layer_8-attn_scores   5.7686\n",
      "   layer_8-attn_out_linear   0.0152\n",
      "          layer_8-attn_out   0.9843\n",
      "           layer_8-ff_h_in   7.6262\n",
      "      layer_8-mlp_dense_in   0.0179\n",
      "        layer_8-mlp_hidden   1.6376\n",
      "     layer_8-mlp_dense_out   0.0172\n",
      "           layer_8-mlp_out   1.1120\n",
      "            layer_8-ff_out   8.2623\n",
      "              layer_9-h_in   1.0056\n",
      "        layer_9-qkv_linear   0.0157\n",
      "                 layer_9-v   1.8151\n",
      "       layer_9-attn_scores   5.2321\n",
      "   layer_9-attn_out_linear   0.0160\n",
      "          layer_9-attn_out   1.0964\n",
      "           layer_9-ff_h_in   7.8038\n",
      "      layer_9-mlp_dense_in   0.0181\n",
      "        layer_9-mlp_hidden   1.6239\n",
      "     layer_9-mlp_dense_out   0.0178\n",
      "           layer_9-mlp_out   1.0790\n",
      "            layer_9-ff_out   8.1446\n",
      "             layer_10-h_in   1.0260\n",
      "       layer_10-qkv_linear   0.0164\n",
      "                layer_10-v   1.8524\n",
      "      layer_10-attn_scores   4.9224\n",
      "  layer_10-attn_out_linear   0.0166\n",
      "         layer_10-attn_out   1.0930\n",
      "          layer_10-ff_h_in   7.8800\n",
      "     layer_10-mlp_dense_in   0.0186\n",
      "       layer_10-mlp_hidden   1.6616\n",
      "    layer_10-mlp_dense_out   0.0186\n",
      "          layer_10-mlp_out   1.1112\n",
      "           layer_10-ff_out   7.8433\n",
      "             layer_11-h_in   1.0206\n",
      "       layer_11-qkv_linear   0.0159\n",
      "                layer_11-v   1.7232\n",
      "      layer_11-attn_scores   3.9257\n",
      "  layer_11-attn_out_linear   0.0163\n",
      "         layer_11-attn_out   1.0256\n",
      "          layer_11-ff_h_in   7.8414\n",
      "     layer_11-mlp_dense_in   0.0186\n",
      "       layer_11-mlp_hidden   1.6669\n",
      "    layer_11-mlp_dense_out   0.0188\n",
      "          layer_11-mlp_out   1.1212\n",
      "           layer_11-ff_out   7.7182\n",
      "             layer_12-h_in   1.0590\n",
      "       layer_12-qkv_linear   0.0162\n",
      "                layer_12-v   1.7409\n",
      "      layer_12-attn_scores   4.7173\n",
      "  layer_12-attn_out_linear   0.0167\n",
      "         layer_12-attn_out   1.0617\n",
      "          layer_12-ff_h_in   8.1189\n",
      "     layer_12-mlp_dense_in   0.0188\n",
      "       layer_12-mlp_hidden   1.6444\n",
      "    layer_12-mlp_dense_out   0.0191\n",
      "          layer_12-mlp_out   1.1245\n",
      "           layer_12-ff_out   7.7900\n",
      "             layer_13-h_in   1.0468\n",
      "       layer_13-qkv_linear   0.0165\n",
      "                layer_13-v   1.8428\n",
      "      layer_13-attn_scores   5.0775\n",
      "  layer_13-attn_out_linear   0.0173\n",
      "         layer_13-attn_out   1.1083\n",
      "          layer_13-ff_h_in   8.0150\n",
      "     layer_13-mlp_dense_in   0.0190\n",
      "       layer_13-mlp_hidden   1.6483\n",
      "    layer_13-mlp_dense_out   0.0195\n",
      "          layer_13-mlp_out   1.1053\n",
      "           layer_13-ff_out   7.5662\n",
      "             layer_14-h_in   1.0712\n",
      "       layer_14-qkv_linear   0.0168\n",
      "                layer_14-v   1.7258\n",
      "      layer_14-attn_scores   3.8857\n",
      "  layer_14-attn_out_linear   0.0176\n",
      "         layer_14-attn_out   1.0911\n",
      "          layer_14-ff_h_in   8.1962\n",
      "     layer_14-mlp_dense_in   0.0189\n",
      "       layer_14-mlp_hidden   1.6071\n",
      "    layer_14-mlp_dense_out   0.0198\n",
      "          layer_14-mlp_out   1.1176\n",
      "           layer_14-ff_out   7.5137\n",
      "             layer_15-h_in   1.0664\n",
      "       layer_15-qkv_linear   0.0167\n",
      "                layer_15-v   1.7072\n",
      "      layer_15-attn_scores   3.8755\n",
      "  layer_15-attn_out_linear   0.0179\n",
      "         layer_15-attn_out   1.0185\n",
      "          layer_15-ff_h_in   8.0909\n",
      "     layer_15-mlp_dense_in   0.0189\n",
      "       layer_15-mlp_hidden   1.6075\n",
      "    layer_15-mlp_dense_out   0.0199\n",
      "          layer_15-mlp_out   1.1277\n",
      "           layer_15-ff_out   7.4398\n",
      "             layer_16-h_in   1.0881\n",
      "       layer_16-qkv_linear   0.0171\n",
      "                layer_16-v   1.7137\n",
      "      layer_16-attn_scores   4.1564\n",
      "  layer_16-attn_out_linear   0.0184\n",
      "         layer_16-attn_out   1.0428\n",
      "          layer_16-ff_h_in   8.3442\n",
      "     layer_16-mlp_dense_in   0.0191\n",
      "       layer_16-mlp_hidden   1.5885\n",
      "    layer_16-mlp_dense_out   0.0203\n",
      "          layer_16-mlp_out   1.1028\n",
      "           layer_16-ff_out   7.4410\n",
      "             layer_17-h_in   1.0600\n",
      "       layer_17-qkv_linear   0.0174\n",
      "                layer_17-v   1.6766\n",
      "      layer_17-attn_scores   3.4033\n",
      "  layer_17-attn_out_linear   0.0194\n",
      "         layer_17-attn_out   0.9701\n",
      "          layer_17-ff_h_in   8.0562\n",
      "     layer_17-mlp_dense_in   0.0190\n",
      "       layer_17-mlp_hidden   1.5716\n",
      "    layer_17-mlp_dense_out   0.0205\n",
      "          layer_17-mlp_out   1.0747\n",
      "           layer_17-ff_out   7.4300\n",
      "             layer_18-h_in   1.0624\n",
      "       layer_18-qkv_linear   0.0178\n",
      "                layer_18-v   1.7248\n",
      "      layer_18-attn_scores   3.1575\n",
      "  layer_18-attn_out_linear   0.0201\n",
      "         layer_18-attn_out   1.0508\n",
      "          layer_18-ff_h_in   8.1728\n",
      "     layer_18-mlp_dense_in   0.0192\n",
      "       layer_18-mlp_hidden   1.5974\n",
      "    layer_18-mlp_dense_out   0.0205\n",
      "          layer_18-mlp_out   1.1209\n",
      "           layer_18-ff_out   7.4202\n",
      "             layer_19-h_in   1.0615\n",
      "       layer_19-qkv_linear   0.0179\n",
      "                layer_19-v   1.7135\n",
      "      layer_19-attn_scores   3.5398\n",
      "  layer_19-attn_out_linear   0.0206\n",
      "         layer_19-attn_out   1.1577\n",
      "          layer_19-ff_h_in   8.2760\n",
      "     layer_19-mlp_dense_in   0.0191\n",
      "       layer_19-mlp_hidden   1.6452\n",
      "    layer_19-mlp_dense_out   0.0208\n",
      "          layer_19-mlp_out   1.1833\n",
      "           layer_19-ff_out   7.5503\n",
      "             layer_20-h_in   1.0767\n",
      "       layer_20-qkv_linear   0.0180\n",
      "                layer_20-v   1.7732\n",
      "      layer_20-attn_scores   4.0424\n",
      "  layer_20-attn_out_linear   0.0199\n",
      "         layer_20-attn_out   1.3028\n",
      "          layer_20-ff_h_in   8.4209\n",
      "     layer_20-mlp_dense_in   0.0194\n",
      "       layer_20-mlp_hidden   1.6984\n",
      "    layer_20-mlp_dense_out   0.0210\n",
      "          layer_20-mlp_out   1.2073\n",
      "           layer_20-ff_out   7.5782\n",
      "             layer_21-h_in   1.0810\n",
      "       layer_21-qkv_linear   0.0184\n",
      "                layer_21-v   1.7785\n",
      "      layer_21-attn_scores   4.0224\n",
      "  layer_21-attn_out_linear   0.0200\n",
      "         layer_21-attn_out   1.1494\n",
      "          layer_21-ff_h_in   8.2744\n",
      "     layer_21-mlp_dense_in   0.0196\n",
      "       layer_21-mlp_hidden   1.7320\n",
      "    layer_21-mlp_dense_out   0.0213\n",
      "          layer_21-mlp_out   1.2651\n",
      "           layer_21-ff_out   7.4656\n",
      "             layer_22-h_in   1.0920\n",
      "       layer_22-qkv_linear   0.0183\n",
      "                layer_22-v   1.7862\n",
      "      layer_22-attn_scores   3.4457\n",
      "  layer_22-attn_out_linear   0.0198\n",
      "         layer_22-attn_out   1.1068\n",
      "          layer_22-ff_h_in   8.3830\n",
      "     layer_22-mlp_dense_in   0.0197\n",
      "       layer_22-mlp_hidden   1.7748\n",
      "    layer_22-mlp_dense_out   0.0215\n",
      "          layer_22-mlp_out   1.3123\n",
      "           layer_22-ff_out   7.4448\n",
      "             layer_23-h_in   1.0679\n",
      "       layer_23-qkv_linear   0.0181\n",
      "                layer_23-v   1.7434\n",
      "      layer_23-attn_scores   4.1805\n",
      "  layer_23-attn_out_linear   0.0192\n",
      "         layer_23-attn_out   0.9471\n",
      "          layer_23-ff_h_in   8.1787\n",
      "     layer_23-mlp_dense_in   0.0192\n",
      "       layer_23-mlp_hidden   1.7087\n",
      "    layer_23-mlp_dense_out   0.0215\n",
      "          layer_23-mlp_out   1.2788\n",
      "           layer_23-ff_out   7.3668\n",
      "             layer_24-h_in   1.0930\n",
      "       layer_24-qkv_linear   0.0187\n",
      "                layer_24-v   1.8122\n",
      "      layer_24-attn_scores   3.7370\n",
      "  layer_24-attn_out_linear   0.0210\n",
      "         layer_24-attn_out   0.9350\n",
      "          layer_24-ff_h_in   8.4118\n",
      "     layer_24-mlp_dense_in   0.0196\n",
      "       layer_24-mlp_hidden   1.7267\n",
      "    layer_24-mlp_dense_out   0.0218\n",
      "          layer_24-mlp_out   1.3129\n",
      "           layer_24-ff_out   7.2809\n",
      "             layer_25-h_in   1.0842\n",
      "       layer_25-qkv_linear   0.0185\n",
      "                layer_25-v   1.7355\n",
      "      layer_25-attn_scores   3.8020\n",
      "  layer_25-attn_out_linear   0.0212\n",
      "         layer_25-attn_out   0.8824\n",
      "          layer_25-ff_h_in   8.2619\n",
      "     layer_25-mlp_dense_in   0.0196\n",
      "       layer_25-mlp_hidden   1.6853\n",
      "    layer_25-mlp_dense_out   0.0219\n",
      "          layer_25-mlp_out   1.3222\n",
      "           layer_25-ff_out   7.0610\n",
      "             layer_26-h_in   1.0808\n",
      "       layer_26-qkv_linear   0.0182\n",
      "                layer_26-v   1.7179\n",
      "      layer_26-attn_scores   3.2287\n",
      "  layer_26-attn_out_linear   0.0227\n",
      "         layer_26-attn_out   0.9180\n",
      "          layer_26-ff_h_in   8.3475\n",
      "     layer_26-mlp_dense_in   0.0201\n",
      "       layer_26-mlp_hidden   1.7206\n",
      "    layer_26-mlp_dense_out   0.0218\n",
      "          layer_26-mlp_out   1.6576\n",
      "           layer_26-ff_out   6.7095\n",
      "             layer_27-h_in   1.0699\n",
      "       layer_27-qkv_linear   0.0167\n",
      "                layer_27-v   2.0565\n",
      "      layer_27-attn_scores   2.5394\n",
      "  layer_27-attn_out_linear   0.0184\n",
      "         layer_27-attn_out   1.4446\n",
      "          layer_27-ff_h_in   8.4017\n",
      "     layer_27-mlp_dense_in   0.0182\n",
      "       layer_27-mlp_hidden   1.6807\n",
      "    layer_27-mlp_dense_out   0.0190\n",
      "          layer_27-mlp_out   2.0745\n",
      "           layer_27-ff_out   7.6421\n",
      "                    logits   1.8646\n"
     ]
    }
   ],
   "source": [
    "mean_scales = scales_df.mean(axis=0)\n",
    "for k in mean_scales.index:\n",
    "    print(f\"{k:>26}\", f\"{mean_scales[k]:8.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
