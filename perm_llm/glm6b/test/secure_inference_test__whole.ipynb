{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from perm_llm.glm6b.wrapped_layer import Attention_GLM_Wrapped, copy_attention, FeedForward_GLM_Wrapped, copy_feedforward\n",
    "from perm_llm.glm6b.utils import generate_position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:11<00:00,  1.45s/it]\n"
     ]
    }
   ],
   "source": [
    "from llm_bases.chatglm6b import ChatGML6B\n",
    "glm = ChatGML6B()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_bases.chatglm6b_official.modeling_chatglm import GLMBlock\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "raw_glm_layers: List[GLMBlock] = glm.condgen.transformer.layers\n",
    "attentions: List[Attention_GLM_Wrapped] = []\n",
    "attentions_public: List[Attention_GLM_Wrapped] = []\n",
    "ffs: List[FeedForward_GLM_Wrapped] = []\n",
    "for i in range(28):\n",
    "    transformer_layer = raw_glm_layers[i].float()\n",
    "    \n",
    "    # The private attention layer\n",
    "    attn_wrapped = Attention_GLM_Wrapped(4096, 32, i)\n",
    "    copy_attention(transformer_layer, attn_wrapped)\n",
    "    attn_wrapped.requires_grad_(False)\n",
    "    attentions.append(attn_wrapped.to(device))\n",
    "\n",
    "    ff_wrapped = FeedForward_GLM_Wrapped(4096, 32, i)\n",
    "    if i == 27:\n",
    "        copy_feedforward(transformer_layer, None, ff_wrapped)\n",
    "        ff_wrapped.layernorm_out = glm.condgen.transformer.final_layernorm.float()\n",
    "    else:\n",
    "        copy_feedforward(transformer_layer, raw_glm_layers[i + 1].float(), ff_wrapped)\n",
    "    ff_wrapped.requires_grad_(False)\n",
    "    ffs.append(ff_wrapped.to(device))\n",
    "\n",
    "word_embedding = glm.condgen.transformer.word_embeddings.weight.float().to(device)\n",
    "input_layernorm = raw_glm_layers[0].input_layernorm.float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from perm_llm.common.communication import Communication, Node, SimulatedCommunication\n",
    "communication = SimulatedCommunication([\"n0\", \"n1\", \"n2\"])\n",
    "communication.new_stage(\"Test\")\n",
    "\n",
    "n0 = Node(communication, \"n0\")\n",
    "n1 = Node(communication, \"n1\")\n",
    "n2 = Node(communication, \"n2\")\n",
    "\n",
    "n1.space.attentions = n2.space.attentions = n0.space.attentions = attentions\n",
    "n0.space.ffs = n1.space.ffs = ffs\n",
    "n0.space.word_embedding = word_embedding\n",
    "n0.space.input_layernorm = n1.space.input_layernorm = input_layernorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from perm_llm.glm6b.secure_inference_utils import generate_scale_dict\n",
    "\n",
    "mask_scale = generate_scale_dict(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "try:\n",
    "    del sys.modules[\"perm_llm.glm6b.secure_inference\"]\n",
    "    del sys.modules[\"perm_llm.glm6b.secure_inference_utils\"]\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from perm_llm.glm6b.secure_inference import GLM_Protocol\n",
    "\n",
    "whole_protocol = GLM_Protocol(n0, n1, n2, mask_scale, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "communication.new_stage(\"prepare\")\n",
    "whole_protocol.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_tensor(query: str):\n",
    "    input_ids, _, _ = glm.get_tokenization(query)\n",
    "    input_ids = input_ids[0]\n",
    "    input_selector = torch.zeros(len(input_ids), glm.n_tokens)\n",
    "    for i in range(len(input_ids)):\n",
    "        input_selector[i, input_ids[i]] = 1\n",
    "    return input_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offline(prompt_len: int, generation_length: int):\n",
    "    for i, next_length in enumerate([prompt_len] + [1] * generation_length):\n",
    "        communication.new_stage(f\"offline_{i}\")\n",
    "        whole_protocol.offline_execute(next_length)\n",
    "\n",
    "\n",
    "def iteratively_generate(query: str, length: int):\n",
    "    input_tensor = get_input_tensor(query).to(device)\n",
    "    generation_start_tensor = input_tensor[-1:]\n",
    "    input_tensor = input_tensor[:-1, :]\n",
    "    generated_ids = []\n",
    "    for i in range(length + 1):\n",
    "        # communication.new_stage(f\"offline_{i}\")\n",
    "        # whole_protocol.offline_execute(input_tensor.shape[0])\n",
    "        communication.new_stage(f\"online_{i}\")\n",
    "        n1.storage[f\"{whole_protocol.name}:x\"] = input_tensor\n",
    "        whole_protocol.online_execute()\n",
    "        if generation_start_tensor is None:\n",
    "            next_id = n1.storage[f\"{whole_protocol.name}:z\"]\n",
    "            generated_ids.append(next_id)\n",
    "            print(i, glm.decode(generated_ids[-1]))\n",
    "            if next_id == glm.condgen.config.eos_token_id:\n",
    "                break\n",
    "            input_tensor = torch.zeros([1, glm.n_tokens]).to(device)\n",
    "            input_tensor[0, next_id] = 1\n",
    "        else:\n",
    "            input_tensor = generation_start_tensor\n",
    "            generation_start_tensor = None\n",
    "    print(glm.decode(generated_ids), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/llm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:717: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1712608935911/work/torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  tensor = as_tensor(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 It\n",
      "2 is\n",
      "3 difficult\n",
      "4 to\n",
      "5 give\n",
      "6 an\n",
      "7 exact\n",
      "8 number\n",
      "9 of\n",
      "10 stars\n",
      "11 in\n",
      "12 the\n",
      "13 sky\n",
      "14 ,\n",
      "15 as\n",
      "16 the\n",
      "17 number\n",
      "18 of\n",
      "19 stars\n",
      "20 in\n",
      "21 the\n",
      "22 universe\n",
      "23 is\n",
      "24 constantly\n",
      "25 changing\n",
      "26 .\n",
      "27 However\n",
      "28 ,\n",
      "29 the\n",
      "30 total\n",
      "It is difficult to give an exact number of stars in the sky, as the number of stars in the universe is constantly changing. However, the total "
     ]
    }
   ],
   "source": [
    "query = \"How many stars are in the sky?\"\n",
    "generation_length = 30\n",
    "offline(len(glm.get_tokenization(query)[0][0]) - 1, generation_length + 1)\n",
    "iteratively_generate(query, generation_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 32, 128])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n0.storage[\"transformer_layer_1/attn/dot_product:beaver_u0 appended, v0, w0\"][-1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NpEncoder, self).default(obj)\n",
    "json.dump(communication.comm_history, open(\"temp/comm_history.json\", \"w\"), indent=4, cls=NpEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_protocol.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Joe\n",
      "2 Biden\n",
      "3 is\n",
      "4 the\n",
      "5 \n",
      "6 4\n",
      "7 6\n",
      "8 th\n",
      "9 President\n",
      "10 of\n",
      "11 the\n",
      "12 United\n",
      "13 States\n",
      "14 ,\n",
      "15 serving\n",
      "16 from\n",
      "17 January\n",
      "18 \n",
      "19 2\n",
      "20 0\n",
      "21 ,\n",
      "22 \n",
      "23 2\n",
      "24 0\n",
      "25 2\n",
      "26 1\n",
      "27 ,\n",
      "28 to\n",
      "29 January\n",
      "30 \n",
      "Joe Biden is the 46th President of the United States, serving from January 20, 2021, to January  "
     ]
    }
   ],
   "source": [
    "query = \"Tell me about Biden\"\n",
    "generation_length = 30\n",
    "offline(len(glm.get_tokenization(query)[0][0]) - 1, generation_length + 1)\n",
    "iteratively_generate(query, generation_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Default',\n",
       " 'Test',\n",
       " 'prepare',\n",
       " 'offline_0',\n",
       " 'offline_1',\n",
       " 'offline_2',\n",
       " 'offline_3',\n",
       " 'offline_4',\n",
       " 'offline_5',\n",
       " 'offline_6',\n",
       " 'offline_7',\n",
       " 'offline_8',\n",
       " 'offline_9',\n",
       " 'offline_10',\n",
       " 'offline_11',\n",
       " 'offline_12',\n",
       " 'offline_13',\n",
       " 'offline_14',\n",
       " 'offline_15',\n",
       " 'offline_16',\n",
       " 'offline_17',\n",
       " 'offline_18',\n",
       " 'offline_19',\n",
       " 'offline_20',\n",
       " 'offline_21',\n",
       " 'offline_22',\n",
       " 'offline_23',\n",
       " 'offline_24',\n",
       " 'offline_25',\n",
       " 'offline_26',\n",
       " 'offline_27',\n",
       " 'offline_28',\n",
       " 'offline_29',\n",
       " 'offline_30',\n",
       " 'offline_31',\n",
       " 'online_0',\n",
       " 'online_1',\n",
       " 'online_2',\n",
       " 'online_3',\n",
       " 'online_4',\n",
       " 'online_5',\n",
       " 'online_6',\n",
       " 'online_7',\n",
       " 'online_8',\n",
       " 'online_9',\n",
       " 'online_10',\n",
       " 'online_11',\n",
       " 'online_12',\n",
       " 'online_13',\n",
       " 'online_14',\n",
       " 'online_15',\n",
       " 'online_16',\n",
       " 'online_17',\n",
       " 'online_18',\n",
       " 'online_19',\n",
       " 'online_20',\n",
       " 'online_21',\n",
       " 'online_22',\n",
       " 'online_23',\n",
       " 'online_24',\n",
       " 'online_25',\n",
       " 'online_26',\n",
       " 'online_27',\n",
       " 'online_28',\n",
       " 'online_29',\n",
       " 'online_30']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(communication.comm_history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json.dump(communication.comm_history, open(\"temp/comm_history.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
